---
title: What is Chalk?
titleHidden: true
hideToc: true
---

import { DocsHero } from '@/components/home/DocsHero'
import { TemporalCorrectness } from '@/components/home/TemporalCorrectness'
import { highlightedCode as addCachingCode } from '@/samples/features/add_caching.py?highlight=diff-py'
import { IntegrationTypes } from '@/components/home/IntegrationTypes'
import { JupyterNotebook } from '@/components/home/Jupyter'
import { RequestingFeaturesOnline } from '@/components/home/RequestingFeaturesOnline'

import { PyDiffEditor } from '@/components/Editor'

<DocsHero />

Chalk enables innovative machine learning teams to focus on building
the unique products and models that make their business stand out.
Behind the scenes, Chalk seamlessly handles data infrastructure with
a best-in-class developer experience. Here’s how it works.

---

Companies are increasingly built not only on data, but on data transformations. Machine Learning models
are the most concrete example of this: they take transformed data (engineered features), and further
"transform" them into outputs—predictions, embeddings, etc. Beyond modeling, companies often need to calculate
metrics and features on the fly (what is perhaps dramatically referred to as "real-time") for their users
or for monitoring purposes, and they need to combine information from heterogenous data sources.

Making transformed features production-grade—that is making them consistent, debuggable, monitorable, quick—and handling
the interplay between increasingly complex and interoperating datasources, is a hard problem, but failing to address
it can cause a lot problems, including: slow model deployment, high query latency,  inefficient and excessive recomputation
of expensive features, diverging implementations of feature processing code, feature drift that goes unnoticed, high storage
overhead, difficulty or inability recovering historic dataset, difficulty versioning features, and difficultly
spotting model performance degradation.

The abstraction that is typically used to "productionize features" is a Feature Store: we will elaborate
later on [what feature storess are](what is a feature store) and [how Chalk is implemented](how chalk works), but
at a high level Chalk, does two things. It centralizes and remembers every feature you've ever computed and it
allows recently calcuated data to be queried in real time with point-in-time correctness guarentees.

Essentially, with Chalk, through a simple python API, you:
1). define your features,
2). connect upstream data sources,
3). specify the desired properties of your features (for instance, Chalk lets you specify [the freshness of a feature](link to feature freshness)
4). write your feature transformation.

Chalk uses the code you write to remembers every computed feature and serves the latest values
for all your features with insane quickness and point-in-time correctness guarentees.

## What is Declarative? And Why Use A Declarative Approach So Good for Feature Storage?

SQL and CSS are two of the most clear examples of declarative programming. In both lanugages,
the desired output not the computation is specified. This allows for a lot of optimization
to happen in the backend to make feature serving wildly fast

  For most comapnies, feature transformation involves some of the following:
- multiple teams writing feature transformation code (for example, data scientists and machine learning
  engineers writing code to preprocess and engineer),
- Batch jobs that are run overnight to generate expensive features,
-

Companies operate on tremendous quantities of data, iterate quickly,
often have multiple teams responsible for engineering features operation and doing it in real time is hard. On top o f 3).
engineered features are often treated as temporary values and are discarded after creation—which is
costly, makes it hard to debug errors, increases the surface space for potential errors. But doing
this right, is critical:

Here are some common pitfalls: features are generating through batch load jobs
are run once a day, which means

Chalk
is a platform that makes this more fluid, though increasingly ubiquitous,

Chalk is a declarative data inteface which enables:
- rapid engineering and testing of new features,
- querying with feature-freshness guarentees,
- querying with point-in-time correctness,
- blazingly fast response times,
- latency monitoring,

Essentially, Chalk is an interface that you place on top of your data layer,
configure with an incredibly simple python interface,

We will expand on what each of these means, but at a high level

both high speed and guaranteed freshness, monitor traffic, latency,
and
point-in-time  consistent features.
Chalk lets you declaratively specify an interface for your data in simple idiomatic python.
This interface provides and unparalleled real time data access, traceability, and guaranteed feature freshness,
Chalk Primer

A chalk data interface can be specified in python through the [chalk python library](link to chalk python library). This python code is pushed to the Chalk Cloud system (or a customer cloud in self-hosted instances), which creates the cloud architecture optimized for the specified interface.

The python code that chalk requires is incredibly simple, but very flexible: it is composed of features and feature resolvers.
What are Features

Features are queryable datasets—you can think of them as tables in in the database that Chalk stitches together based on the declarative specifications of your chalk code.

For instance, a "Parks" Feature derived from the national parks api, might look like the following.

from chalk.features import feature

@features
class Park:
  id: int
  full_name: str

Essentially, Features specify what you want your data to look like. They are define what you can query through chalk's clients. Features are complemented by Feature Resolvers.
What are Feature Resolvers

Resolvers specify how a feature is derived. Feature resolvers are incredibly versatile: they can can pull features directly from a database, they can create features from the outputs of other resolvers, they can invoke arbitrary python transformations.

Feature resolvers also specify the desired characteristics of a feature, such as the max staleness of its values—this is where chalk shines; it provides a clean interface for specifying feature characteristics with high granularity. Feature resolvers are python functions that take zero or more Features (or Feature subsets) as inputs and output new features. Below is an example of a feature resolver that takes no inputs and populates the "Park" feature.

    Note

    The feature resolver returns a DataFrame of Parks. DataFrames are how chalk specifies feature resolvers that return multiple rows of a feature.

    Note The decorator on the get_parks feature resolver specifies that whenever an end user queries the features table, the function needs to be rerun—get_parks cannot rely on cached data.

from chalk.features import DataFrame
@online
def get_parks() -> DataFrame[Park]:
  return [
    Park(id=2, fullName='Yellowstone'),
    Park(id=3, fullName='Yosemite'),
    Park(id=4, fullName='Zion'),
  ]

Building a Simple Chalk Data Interface

To build a simple chalk data interface for the national parks API, we will:

    Download and setup the chalk toolkit,
    Write a Feature,
    Write a Feature Resolver,
    Push our code to chalk.

We will go through a couple iterations of the last few steps to incrementally understand how chalk works.

## Develop

Chalk makes it simple to develop feature pipelines for machine
learning. Define Python functions using the libraries and tools you're
familiar with instead of specialized DSLs. Chalk then orchestrates
your functions into pipelines that execute in parallel on a Rust-based
engine and coordinates the infrastructure required to compute
features.

### Features

To get started, [define your features](/docs/features) with
[Pydantic](https://pydantic-docs.helpmanual.io/)-inspired Python classes.
You can define schemas, specify relationships, and add metadata
to help your team share and re-use work.

```py
@features
class User:
    id: int
    full_name: str
    nickname: str | None
    email: str | None
    birthday: datetime
    credit_score: float
    datawarehouse_feature: float

    transactions: DataFrame[Transaction] = has_many(
        lambda: Transaction.user_id == User.id
    )
```

### Resolvers

Next, tell Chalk how to compute your features.
Chalk ingests data from your existing data stores,
and lets you use Python to compute features with
[feature resolvers](/docs/resolver-overview).
Feature resolvers are declared with the decorators `@online` and
`@offline`, and can depend on the outputs of other feature resolvers.

Resolvers make it easy to rapidly integrate a wide variety of data
sources, join them together, and use them in your model.

<IntegrationTypes />

---

## Execute

Once you've defined your features and resolvers, Chalk orchestrates
them into flexible pipelines that make training and executing models easy.

Chalk has built-in support for feature engineering workflows --
there's no need to manage Airflow or orchestrate complicated streaming flows.
You can execute resolver pipelines with declarative caching,
ingest batch data on a schedule, and easily make slow sources
available online for low-latency serving.

### Caching

Many data sources (like vendor APIs) are too slow for online use cases
or charge a high dollar cost-per-call. Chalk lets you optimize latency
and cost by defining declarative caching policies that are well-integrated
throughout the system. You no longer have to manage data sources such as
Redis, Memcached, or DynamoDB, or spend time tuning cache-warming pipelines.

Add a caching policy with one line of code in your feature definition:

<PyDiffEditor html={addCachingCode} />

Optionally warm feature caches by executing resolvers on a schedule:

```py
@online(cron="**1d**")
def fn(id: User.id) -> User.credit_score:
  return redshift.query(...).all()
```

Or override staleness tolerances at query time when you need fresher
data for your models:

```py
chalk.query(
    ...,
    outputs: [User.fraud_score],
    max_staleness: { User.fraud_score: "1m" }
)
```

### Batch ETL ingestion

Chalk also makes it simple to generate training sets from data warehouse
sources -- join data from services like S3, Redshift, BQ, Snowflake
or other custom sources with historical features computed online.
Specify a cron schedule on an `@offline` resolver, and Chalk will automatically ingest
data with support for incremental reads:

```py
@offline(cron="**1h**")
def fn() -> Feature[User.id, User.datawarehouse_feature, User.updated_at]:
  return redshift.query(...).incremental()
```

Chalk makes this data available for point-in-time-correct dataset
generation for data science use cases. Every pipeline has built-in
monitoring and alerting to ensure data quality and timeliness.

### Reverse ETL

When your model needs to use features that are canonically stored in
a high-latency data source (like a data warehouse), Chalk's Reverse
ETL support makes it simple to bring those features online and serve
them quickly.

Add a single line of code to an `offline` resolver, and Chalk constructs
a managed reverse ETL pipeline for that data source:

```py
@offline(offline_to_online_etl="5m")
```

Now data from slow offline data sources is automatically available for
low-latency online serving.

---

## Deploying and querying

Once you've defined your pipelines, you can rapidly deploy them to
production with Chalk's CLI:

```bash
chalk apply
```

This creates a deployment of your project, which is served at a unique
preview URL. You can promote this deployment to production, or
perform QA workflows on your preview environment to make sure that
your Chalk deployment performs as expected.

Once you promote your deployment to production, Chalk makes features
available for low-latency [online inference](/docs/query-basics) and
[offline training](/docs/training-client). Chalk uses
the **exact same source code** to serve temporally-consistent training
sets to data scientists and live feature values to models. This re-use
dramatically shortens development time and ensures that feature values
from online and offline contexts match.

### Online inference

Chalk's online store & feature computation engine make it easy to query
features with ultra low-latency for your online inference use cases.

Integrating Chalk with your production application takes minutes via
Chalk's simple REST API:

<RequestingFeaturesOnline style={{ height: 328 }} />

Features computed to serve online requests are also replicated to Chalk's
offline store for historical performance tracking and training set generation.

### Offline training

Data scientists can use Chalk's Jupyter integration to create datasets
and train models. Datasets are stored and tracked so that they can be
re-used by other modelers. Chalk implements model provenance to track
inputs, outputs, and other data for audit and reproducibility.

<JupyterNotebook />

Chalk datasets are always "temporally consistent."
This means that you can provide labels with different past timestamps to easily
get historical features that represent what your application would have
retrieved online at those past times. Temporal consistency ensures that
your model training doesn't mix "future" and "past" data.

<TemporalCorrectness />
