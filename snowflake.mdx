---
title: Snowflake
description: Integrate with Snowflake.
published: true
---

import { getEnvironmentVariableText, getNamedIntegrationText } from "@/components/NamedIntegration";
import { AddSnowflakeIntegration } from '@/components/shared/snowflake'

---

Chalk supports [Snowflake](https://www.snowflake.com/) as a [SQL source](/docs/sql).
You can configure the Snowflake-specific options using the `SnowflakeSource.__init__` args.
Alternately, you can configure the source through your dashboard.

## From the dashboard

In the dashboard, you can define the Snowflake Source along with some default 
configurations, such as the database, schema, and warehouse. If the role that you 
have provided has permissions to, you would be able to use the same Snowflake 
source to query other tables, but if not otherwise specified, Chalk will use the 
defaults set in the data source definition. 

<AddSnowflakeIntegration onSubmit={(e) => e.preventDefault()} />

---

## Single Integration

If you have only one Snowflake connection that you'd like
to add to Chalk, you do not need to specify any arguments
to construct the source in your code.

```py
from chalk.sql import SnowflakeSource

snowflake = SnowflakeSource()

@online
def fn(...) -> ...:
    return snowflake.query(...).first()
```


## Multiple Integrations

<span>{getNamedIntegrationText()}</span>

```py
from chalk.sql import SnowflakeSource

risk = SnowflakeSource(name="**RISK**")
marketing = SnowflakeSource(name="**MARKETING**")

@online
def risk_resolver(...) -> ...:
    return risk.query(...).first()

@online
def marketing_resolver(...) -> ...:
    return marketing.query(...).first()
```

<span>{getEnvironmentVariableText("RISK", "SNOWSQL_SCHEMA")}</span>

## Environment Variables

You can also configure the integration directly using environment variables
on your local machine or from those added through the
[generic environment variable](/docs/env-vars) support.

```py
import os
from chalk.sql import SnowflakeSource

snowflake = SnowflakeSource(
    db=os.getenv("SNOWSQL_DATABASE"),
    schema=os.getenv("SNOWSQL_SCHEMA"),
    role=os.getenv("SNOWSQL_ROLE"),
    warehouse=os.getenv("SNOWSQL_WAREHOUSE"),
    user=os.getenv("SNOWSQL_USER"),
    password=os.getenv("SNOWSQL_PWD"),
    account_identifier=os.getenv("SNOWSQL_ACCOUNT_IDENTIFIER")
)

@online
def resolver_fn(...) -> ...:
    return snowflake.query(...).first()
```

## Required Permissions

To use Snowflake as a Chalk datasource, the user or role specified in your integration must have the following permissions:

### Core Query Permissions

- `USAGE` on the warehouse
- `USAGE` on the database
- `USAGE` on the schema
- `SELECT` on all tables and views being queried

### Temporary Table Operations

Chalk creates temporary tables during feature computation, so the role also needs:

- `CREATE TEMPORARY TABLE` in the schema
- `DROP` on temporary tables in the schema

### Data Unloading (Optional)

If you're using Chalk's data unload optimization to stages, grant:

- `USAGE` on the stage
- `READ` on the stage
- `WRITE` on the stage

### Recommended RBAC Setup

Here's an example of setting up a role with the appropriate permissions:

```sql
-- Create a role for Chalk
CREATE ROLE IF NOT EXISTS CHALK_ROLE;

-- Grant warehouse permissions
GRANT USAGE ON WAREHOUSE <warehouse_name> TO ROLE CHALK_ROLE;

-- Grant database/schema permissions
GRANT USAGE ON DATABASE <database_name> TO ROLE CHALK_ROLE;
GRANT USAGE ON SCHEMA <database_name>.<schema_name> TO ROLE CHALK_ROLE;
GRANT SELECT ON ALL TABLES IN SCHEMA <database_name>.<schema_name> TO ROLE CHALK_ROLE;

-- Grant temporary table permissions
GRANT CREATE TEMPORARY TABLE ON SCHEMA <database_name>.<schema_name> TO ROLE CHALK_ROLE;

-- Grant stage permissions (if using data unloading)
GRANT USAGE ON STAGE <stage_name> TO ROLE CHALK_ROLE;
GRANT READ, WRITE ON STAGE <stage_name> TO ROLE CHALK_ROLE;

-- Assign role to user
GRANT ROLE CHALK_ROLE TO USER <chalk_user>;
```

## Creating a Stage for Data Unloading

Chalk can optimize large query result transfers by unloading data to a Snowflake stage
and reading it from cloud storage (GCS or S3). This is particularly useful for large
feature computations.

### Create a Named Stage

First, create a stage in your Snowflake account:

```sql
-- Create an internal named stage
CREATE OR REPLACE STAGE <stage_name>;

-- Or create an external stage (e.g., for S3)
CREATE OR REPLACE STAGE <stage_name>
  URL = 's3://<bucket>/<path>'
  CREDENTIALS = (AWS_KEY_ID = '<key>' AWS_SECRET_KEY = '<secret>');
```

### Configure the Stage Path

After creating a stage, you'll see output like:

```
@CHALK_UNLOAD_STAGE
```

To enable data unloading in your Chalk features, add this stage reference to your environment variables
in the Chalk UI. Go to **Settings â†’ Variables** and add:

```
CHALK_SNOWFLAKE_UNLOAD_STAGE=<CHALK_UNLOAD_STAGE>
```

(Replace `<CHALK_UNLOAD_STAGE>` with your actual stage name.)

Chalk will use this stage to unload and transfer large result sets efficiently, improving performance
for large feature computations.
