---
title: Python Resolvers
description: What are python resolvers.
---

Python resolvers run python code to transform and combine your features into
new features. A large portion of your resolvers will likely be python resolvers.
In them, you can run arbitrary python code: for example, you can make API requests,
run model predictions, or perform Dataframe operations and aggregations.

Below, is a simple example of a python resolver that takes a user's email and
extracts the domain.

```
from chalk.features import features, online

@features
class User:
    id: str
    email: str
    email_domain: str

@online
def get_domain(email: User.email) -> User.email_domain:
    # type(email) == str
    return email.split('@')[1].lower()
```

In this section we'll go through [how to specify inputs](#inputs) to your python
resolvers, [how to specify outputs](#outputs), and [how to specify run conditons](#run-conditions).

## Inputs

Python resolvers typically require input features to compute their
output features (normally, you'll use [SQL resolvers](/docs/resolver-sql)
as your entry point resolvers, pulling data from your data sources directly
into your features).

Resolver dependencies are declared through the type signature of the
arguments to the resolver function. In the above example,
the `get_domain` resolver takes a single argument, `email`, which is
of type `User.email`. The above example, shows a relatively simple
input, but resolver inputs can get pretty complex. We'll cover the
following types of inputs:
- scalar dependencies: any primitive python type,
- has-one dependencies: relationships to other feature sets,
    - scalar has-one: relationships to scalar features,
    - optional has-one: relationships that may not exist,
    - nested has-one: relationships that traverse through relationships,
- has-many dependencies
- filtering DataFrame inputs by time
- Now, explicitly time-dependent resolvers

### Scalar dependencies

To depend on features from a [Feature Set](/docs/features),
annotate your resolver arguments with your features as the types.

You can then use those arguments in the body of your resolver to
compute your [output features](/docs/resolver-outputs).

```py
from chalk.features import features, online

@features
class User:
    id: str
    email: str
    email_domain: str

@online
def get_domain(email: User.email) -> User.email_domain:
    # type(email) == str
    return email.split('@')[1].lower()
```

You can require multiple features in a resolver.
However, all feature dependencies in a single resolver
need to originate in the same root namespace:

<TipGood>Requiring features from the same root namespace</TipGood>

```py
@online
def fn(a: User.email, b: User.name) -> User.email_name_match:
    return ...
```

Here, we incorrectly request features from the root namespaces
of `Transfer` and `User`:

<TipBad>Requiring features from different root namespaces</TipBad>

```py
@online
def fn(a: User.email, b: Transfer.memo) -> Transfer.email_in_memo:
    return email in memo
```

If you want to require features from namespaces,
you can use [has-one](/docs/has-one) or
[has-many](/docs/has-many) relationships.

<TipGood>Requiring features from different root namespaces using a relationship</TipGood>

```py
@online
def fn(email: Transfer.user.email, memo: Transfer.memo) -> Transfer.email_in_memo:
    return email in memo
```

---

### Has-one dependencies

#### Scalar has-one

You can also require features joined to a
[Feature Set](/docs/features) through
[has-one](/docs/has-one) relationships.
For example, if users in your system have bank accounts,
and you wanted to compare the name on the user's bank account
to the user's name, you could require the user's name and
the account's title through the user:

```py
@online
def name_sim(title: User.account.title, name: User.full_name) -> ...
```

You can also require all scalars on the user's profile:

```py
@online
def fn(profile: User.profile) -> ...:
    profile.signup_date
```

Chalk will materialize all scalar features on the profile
before calling this function.
If you want to pull only a few features from the profile,
require each directly:

```py
@online
def fn(signup_date: User.profile.signup_date, age: User.profile.age) -> ...
```

#### Optional has-one

Has-one relationships can also be
[declared as optional](/docs/has-one#optional-relationships).
You may also require feature through optional relationships,
but the types for all of those optional features will become
optional. Consider the below example:

```py
@features
class Account:
    id: int
    user_id: int
    balance: float  # Non-optional balance

@features
class User:
    id: int
    # Optional relationship
    account: Account | None = has_one(lambda: Account.user_id == User.id)
    has_high_balance: bool

@online
def has_high_bal(balance: User.account.balance) -> User.has_high_balance:
    # Balance will be "float | None"
    if balance is None:
        return False
    return balance > 1000

```

The resolver in this example receives an optional `float`,
even though `balance` is not an optional field on `Account`.
The optional is added because the user may not have an account,
in which case the resolver will receive `None` for the balance.

#### Nested has-one

You can also traverse nested [has-one](/docs/has-one)
relationships in the same manner as
[requiring a single has-one](#has-one-dependencies).

Consider a schema where users have a feature set of profile information,
and the user's profile has an identity feature set, which in turn
has the age of the user's email.
You can require the email age feature as below:

```py
@online
def fn(email_age: User.profile.identity.email_age) -> ...
```

However, you cannot access nested relationships without
explicit asking for them.

<TipBad>Accessing a transitive relationship from a dependency.</TipBad>

```py
@online
def fn(acct: User.account) -> ...:
    acct.balance           # Ok
    acct.institution.name  # Error!
```

Instead, you can require the nested relationship directly
and access any of its scalar features.

<TipGood>Directly requiring the transitive relationship.</TipGood>

```py
@online
def fn(ins: User.account.institution, acct: User.account) -> ...:
    acct.balance  # Ok
    ins.name      # Ok
```

The semantics of
[optional has-one dependencies](#optional-has-one)
carry over to nested has-one dependencies.
If you traverse an optional relationship,
then all downstream attributes will become optional.

---

### Has-many dependencies

#### Scalar has-many

You can also require [has-many relationships](/docs/has-many)
as inputs to your resolver:

```py
@online
def fn(transfers: User.transfers) -> ...:
```

You receive a [Chalk DataFrame](/docs/dataframe),
which supports
[projections](/docs/dataframe#projections),
[filtering](/docs/dataframe#filters), and
[aggregations](/docs/dataframe#aggregations),
among other operations.

#### Projections

By default, Chalk will materialize all scalar features
on the `Transfer` feature set before calling your resolver.
As an optimization hint, you can specify which features from
the transfers that you'd like Chalk to materialize before calling
the function. For example, if there were expensive features
to compute on the transfer, you could scope the features
to only the set you need:

```py
@online
def fn(transfers: User.transfers[Transfer.amount, Transfer.memo]) -> ...:
    transfers[Transfer.amount].sum()      # Ok
    transfers[Transfer.from_institution]  # Error: filtered out above
```

The error above is surfaced statically
by our [editor plugin](/docs/editor-setup).


#### Filtering

You can apply filters to the has-many inputs of resolvers:

```py
@online
def fn(transfers: User.transfers[Transfer.amount > 100]) -> ...:
```

[Filters](/docs/dataframe#filters) can be composed with
[projections](/docs/dataframe#projections) following the semantics
of the Chalk `DataFrame`.

```py
@online
def fn(transfers: User.transfers[Transfer.amount > 100, Transfer.memo]) -> ...:
```

Filters can also be used on features through has-one relationships.

```py
@online
def fn(transfers: User.transfers[Transfer.amount, Transfer.bank.location == "USA"]) -> ...:
```

More on projection and filtering [here](/docs/dataframe#composing-projections-and-filters)

#### Has-many through has-one

Has-many relationships can be required through has-one relationships:

```py
@online
def fn(transfers: User.account.transfers) -> ...:
```

As with [scalar has-many dependencies](#scalar-has-many),
you can scope down the scalar features on the transfer
to only those required:

```py
@online
def fn(transfers: User.account.transfers[Transfer.amount]) -> ...:
    transfers[Transfer.amount].sum()      # Ok
    transfers[Transfer.from_institution]  # Error: filtered out above
```

#### Has-many through optional-has-one

If the has-one relationship that you're traversing is
[optional](/docs/has-one#optional-relationships),
then the `transfers` argument in the example above will
either be `None` or a [Chalk DataFrame](/docs/dataframe).

#### Has-one through has-many

You can also select columns through nested has-one relationships that would not normally materialize.

```py
@online
def fn(transfers: User.transfers[Transfer.amount, Transfer.bank.name]) -> ...:
```

In the above example, if there was a has-one relationship between `Transfer` and `Bank`,
we can fetch any scalar feature from `Bank` as well for the `DataFrame`.
Note that the `Bank.name` column would _not_ materialize with the simple `transfers: User.transfers` typing,
since this typing only materializes scalar features in the root namespace.

#### Has-many through has-many

Has-many relationships can be required through other has-many relationships.

For example, consider the following feature definitions for `User`, `Account`, and `Transaction`,
where a user can have many accounts, each with many transactions.

```py
from chalk.features import features, has_many, DataFrame


@features
class Transaction:
    id: str
    account_id: str
    amount: float


@features
class Account:
    id: str
    user_id: str
    transactions: DataFrame[Transaction] = has_many(
        lambda: Transaction.account_id == Account.id
    )


@features
class User:
    id: str
    total_spent: float
    accounts: DataFrame[Account] = has_many(lambda: Account.user_id == User.id)
```

We can resolve the `total_spent` feature on `User` by computing the sum of transaction
amounts across all of a user's accounts, as shown below.

```python
@online
def get_total_spent(
    txns: User.accounts.transactions[Transaction.amount]
) -> User.total_spent:
    return txns.sum()
```

### Time Filtering

Window functions are helpful for creating features that describe
recent behavior.
With window functions, you can implement features such as moving
averages or counts of events in a window.

Window functions are computed on a [DataFrame](/docs/dataframe).
A [has-many](/docs/has-many) feature creates a [DataFrame](/docs/dataframe)
on a feature set, which you can filter with the special functions
`before` and `after`.
These functions filter your data relative to the
current time in context. This time could be in the past
if you're using an offline resolver.
Using window functions ensures that you maintain
[point-in-time correctness](/docs/temporal-consistency).

Chalk supplies two functions, `before` and `after`,
which take many keyword arguments describing the time
relative to the present:

```py
# Keep where over three years and two days old
before(years_ago=3, days_ago=2)

# Keep where under 5:10 minutes old
after(minutes_ago=5, seconds_ago=10)
```

You can use these operators to filter out rows of a `DataFrame`:

#### After

To compute the number of transfers a user made in the
last seven days, you can use the `after(...)` function:

```py
from chalk.features import after, ...

@online
def fn(transfers: User.transfers[after(days_ago=7)]) -> ...:
    return transfers.count()
```

#### Before

Alternatively, if you wanted to compute the number of transfers
a user made more than seven days ago, you would write:

```py
from chalk.features import before, ...

@online
def fn(transfers: User.transfers[before(days_ago=7)]) -> ...:
    return transfers.count()
```

#### Combining before and after

You can also combine `before` and `after`.
In this example, we return the transfers made over a week ago,
but less than two weeks ago.
These filters can also be combined with other filters and projections
as described [here](/docs/dataframe#composing-projections-and-filters).

```py
from chalk.features import before, after, ...

@online
def fn(transfers: User.transfers[after(days_ago=14), before(days_ago=7)]) -> ...:
    return transfers.count()
```

### Now: Explicitly time-dependent resolvers

Chalk supports resolvers that are explicitly time-dependent. This is useful for performing backfills which
compute values that depend on timestamps that are semantically similar to `datetime.now()`.

You can express time-dependency by declaring a dependency on a special feature called `Now`,
which gets converted into a `datetime` within resolvers:

```py
@online
from chalk.features import Now

def get_age_in_years(birthday: User.birthday, now: Now) -> User.age_in_years:
    return (now - birthday).years
```

Note, in online queries, (i.e. with `ChalkClient().query`), `Now` is `datetime.now()` if the `now` parameter is unused.
In offline query contexts, `now` will be set to the appropriate `input_time` value for the calculation.
This allows you to backfill a feature for a single entity at many different historical time points:

```py
ChalkClient().offline_query(input={User.id: [1,1,1]}, output=[User.age_in_years], input_times=[
    datetime.now() - timedelta(days=365*10),
    datetime.now() - timedelta(days=365),
    datetime.now() - timedelta(days=0),
])

## output:

# | id | age_in_years |
# | 1  | <age> - 10   |
# | 1  | <age> - 1    |
# | 1  | <age> - 0    |
```

`Now` can be used in DataFrame resolvers as well in order to compute bulk values:

```py
@online
def batch_get_age_in_years(df: DataFrame[User.id, User.birthday, Now]) -> DataFrame[User.id, User.age_in_years]:
    return (
        df.to_polars()
            .select(
                pl.col(User.id),
                pl.col(str(User.birthday) - pl.col(str(Now))).alias(str(User.age_in_years))
            )
    )
```

## Outputs

Resolvers declare the features that they resolve through a [Python type annotation](https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html#functions)
on the return value of the function.

Python resolvers can return the following types of outputs:
- single features,
- multiple features,
- feature sets,
- multiple features sets.

### Single features

To return a single feature from a resolver,
set the return type annotation to the feature you
want to resolve:

```py
from chalk.features import features

@features
class User:
    id: int
    name: str
    employer: str

@online
def resolve(u: User.id) -> User.name:
    return "Jennifer Doudna"
```

Equivalently, you can wrap the return value in the `User` class:

```diff-py
@online
def resolve(u: User.id) -> User.name:
-   return "Jennifer Doudna"
+   return User(name="Jennifer Doudna")
```

### Multiple features

To return multiple features, return an [instance of the feature class](/docs/features#constructing-feature-classes).
In the type signature, specify the `Features[...]` class, parameterized
by the features that you pass to the feature class.

```diff-py
from chalk.features import Features

@online
def resolve(u: User.id) -> Features[User.name, User.employer]:
    return User(
        name="Jennifer Doudna",
        employer="University of California, Berkeley"
    )
```

You only need to pass [a subset of the features](/docs/features#constructing-feature-classes)
to the constructor for the feature class.

### All features

To return all features of a class, you can just use feature set as your output,
like so:

```py
@online
def get_user(u: User.id) -> User:
    return User(
        name="Jennifer Doudna",
        employer="University of California, Berkeley"
    )
```

### Almost All Features

You may want to return _almost_ all features of a class.
Writing out all the features can be tedious and error-prone.
You can subtract features from a feature class
using the `-` operator:

```py
from chalk import online
from chalk.features import Features

@online
def get_all_users() -> Features[User] - User.name:
    return User(id=5, employer="University of California, Berkeley")
```

However, here, only the `name` feature is not returned, which leaves both the
`employer` and id feature.


## DataFrame output

You can also output many instances of a feature set from a resolver
by specifying a [DataFrame](/docs/dataframe) as the return type of
the function:

```py
@offline
def get_events() -> DataFrame[Transfer.uuid, Transfer.amount, Transfer.ts]:
    return DataFrame.read_csv(...)
```

For more info on how to load batch data,
see the [Data Sources](/docs/sql) sections.
`DataFrame`-returning resolvers don't need inputs.


### All features

To return all features of a class in a DataFrame,
use `DataFrame[...]` class around the feature class:

```py
@online
def get_all_users() -> DataFrame[User]:
    return DataFrame([
        User(
            name="Jennifer Doudna",
            employer="University of California, Berkeley"
        )
    ])
```

### Other DataFrame-returning resolvers

Imagine a scalar feature you'd like to backfill over many thousands of pkeys and historical times.
`DataFrame`-returning resolvers can dramatically reduce the computation time due to its vectorized handling.

```py
@offline
def get_new_feature_as_dataframe(
    df: DataFrame[Transaction.id, ...]
) -> DataFrame[Transaction.id, Transaction.new_feature]:
```

The above resolver runs faster on a thousand rows
than the equivalent scalar resolver ran a thousand times.

Chalk also supports relationship-returning resolvers that enable users to
return a `DataFrame` belonging to a has-many relationship.

```py
@offline
def relationship_returning_resolver(
    df: User.transactions[Transaction.id, Transaction.amount, Transaction.description],
    user_type: User.type
) -> User.transactions[Transaction.id, Transaction.transaction_type]:
```

Just make sure that the return `DataFrame`s do not have duplicate rows.
That means no two rows should have the same primary key, or primary key & timestamp combinations if the
feature time is also returned.

### Overriding Output Time

By default, features are timestamped with the execution time of their resolver. However, you can override this behavior by providing timestamps from your data source This functionality can be helpful when working with an event store or timestamped API.
The first step is to [add a FeatureTime or `ts` feature to your feature set](/docs/feature-types#feature-time).

You can then run resolver which sets the feature time of your

## Run Conditions

Both offline and online resolvers [support scheduled runs](/docs/resolver-cron).
However, you're likely to consider scheduling more frequently with offline resolvers.
Every offline resolver pulls in data from your underlying source to
Chalk's feature store on a schedule.
Chalk will determine the frequency at which to poll your data sources,
or you can choose to provide a custom [duration](/docs/duration) on which
to pull the data.

Online resolvers can also be scheduled, although they don't _necessarily_
run on a schedule as offline resolvers do.

Both online and offline scheduled resolvers may take arguments,
and you have
[control over the sets of arguments](/docs/resolver-cron#filtering-examples)
to run.


staging` environment.

## Restricting Resolver Execution By Environment

Environments are used to trigger behavior in different deployments
such as staging, production, and local development.
For example, you may wish to interact with a vendor via an API call
in the `production` environment, and opt to return a constant value
in a `staging` environment.

## Specifying environments

You can choose to scope resolvers to a restricted set of environments.
Resolvers optionally take a keyword argument named `environment`
that can take one of three types:

- **Unassigned (default)** - The resolver will be a candidate to run in _every_ environment.
- **String value** - The resolver will run _only_ in this environment.
- **List of strings** - The resolver will run in _any_ of the specified environments and no other environments.

## Example

Say your fraud models needed to interact with a fraud vendor that you wanted
to mock out in staging. We can scope the environments as follows:

```python
@online(environment="**production**")
def fraud_score_prod(email: User.email, phone: User.phone) -> User.fraud_score:
    return api_vendor.fraud_score(email)

@online(environment=["**staging**", "**dev**"])
def fraud_score_staging(email: User.email) -> User.fraud_score:
    if email == "fraud_user@chalk.ai":
        return 10
    return 90
```

Resolvers in different environments don't need to take the same arguments.
In the above example, the production version of the resolver takes a phone
number and an email, while the staging version of the resolver takes only
the email.

## Restricting Resolver Execution Conditions With Tags

Tags can be either a string value, or a key-value pair.
As a best practice
(and fitting with recommendations from other services like
[Datadog](https://docs.datadoghq.com/getting_started/tagging/#unified-service-tagging)),
we recommend using key-value pairs, but the choice is yours.

There are two ways to specify tags on resolvers:

- **<code className="before:content-none after:content-none">"key"</code>** -
As a single scalar string.
- **<code className="before:content-none after:content-none">"key:value"</code>** -
As a string value representing a key-value pair.

Resolvers take one or many tags, all of which need to match for the
resolver to run. For example, you may want to test with
a sandboxed vendor, and also be able to set a constant value
for a particular feature.

```py
@online(tags=["**api:mock**", "**fraud**"])
def simulate_fraud(id: User.id) -> User.fraud_score:
    return 100

@online(tags="**api:mock**")
def simulate_no_fraud(id: User.id) -> User.fraud_score:
    return 10

@online(tags="**api:sandbox**")
def sandbox_score(
    name: User.name,
    email: User.email,
) -> User.fraud_score:
    return sandbox.fraud_score(name, email).score

@online
def real_score(
    name: User.name,
    email: User.email,
) -> Features[User.fraud_score, User.fraud_tags]:
    r = prod.fraud_score(name, email).score
    return User(fraud_score=r.score, fraud_tags=r.tags)
```

In the above example, the resolver that is chosen to compute
the `User.fraud_score` feature will depend on the tags provided
at query time. The table below shows which resolver will be
chosen for a given set of tags.

| Query tags                                                              | Resolver                                                             |
| ----------------------------------------------------------------------- | -------------------------------------------------------------------- |
| <span className="font-mono text-accent-teal"> [api:mock, fraud] </span> | <span className="font-mono text-pink-500"> simulate_fraud </span>    |
| <span className="font-mono text-accent-teal"> api:mock </span>          | <span className="font-mono text-pink-500"> simulate_no_fraud </span> |
| <span className="font-mono text-accent-teal"> api:sandbox </span>       | <span className="font-mono text-pink-500"> sandbox_score </span>     |
| <span className="font-mono text-accent-teal"> &lt;otherwise&gt; </span>       | <span className="font-mono text-pink-500"> real_score </span>        |

Note that these resolvers don't need to take the same set of inputs,
and don't need to return the same types.

## When tagged resolvers run

Like [Environments](/docs/resolver-environments), tags control when resolvers run
based on the
[Online Context](/docs/query-basics) or [Training Context](/docs/training-client)
matching the tags provided to the resolver decorator.
Resolvers optionally take a keyword argument named `tags`
that can take one of three types:

- **Unassigned (default)** - The resolver will be a candidate to run for _every_ set of tags.
- **String value** - The resolver will run _only_ if this tag is provided.
- **List of strings** - The resolver will run _only_ if _all_ of the specified tags match.

If your resolver is tagged only by a key (not a key-value pair),
and the request context contains a key-value pair such that the resolver's
tag (a key only) matches they key of a key-value pair in the context,
the resolver will be eligible to run. For example:

| Resolver Tag                                                                 | Request Context                                                           | Matches? |
| ---------------------------------------------------------------------------- | ------------------------------------------------------------------------- | -------- |
| <span className="font-mono text-accent-teal"> api </span>                    | <span className="font-mono text-pink-500"> api </span>                    | Yes      |
| <span className="font-mono text-accent-teal"> api </span>                    | <span className="font-mono text-pink-500"> api:live </span>               | Yes      |
| <span className="font-mono text-accent-teal"> [api:live, mock-phone] </span> | <span className="font-mono text-pink-500"> [api:live, mock-phone] </span> | Yes      |
| <span className="font-mono text-accent-teal"> [api:live, mock-phone] </span> | <span className="font-mono text-pink-500"> api:live </span>               | No       |
| <span className="font-mono text-accent-teal"> api:live </span>               | <span className="font-mono text-pink-500"> api </span>                    | No       |
| <span className="font-mono text-accent-teal"> api:fixture </span>            | <span className="font-mono text-pink-500"> api:live </span>               | No       |

## Example

Frequently, you'll want to combine tags and [Environments](/docs/resolver-environments),
as below.
This span uses a constant value in staging when the tag `api` takes the value `fixture`,
uses the sandboxed fraud vendor in staging when the tag `api` takes the value `live`,
and uses the production fraud vendor in production.

```python
@online(environment="staging", tags="**api:fixture**")
def fraud_score_fixture(email: UserFeatures.email) -> UserFeatures.fraud_score:
    if email == "elliot@chalk.ai":
        return 100
    return 50

@online(environment="staging", tags="**api:live**")
def fraud_score_sandbox(email: UserFeatures.email) -> UserFeatures.fraud_score:
    return api_vendor_sandbox.fraud_score(email, profile="dev")

@online(environment="production")
def fraud_score_prod(email: UserFeatures.email) -> UserFeatures.fraud_score:
    return api_vendor_live.fraud_score(email)
```

## Resolver crons

You can schedule resolvers to run on a pre-determined schedule
via the `cron` argument to resolver decorators.

```py
@online(cron="**90d**")
def fn() -> DataFrame[User.id, User.name]:
    return ...
```

The `cron` keyword arguments takes a [duration](/docs/duration)
to determine the schedule on which to run.  For more fine-grained control,
you can alternatively specify a [crontab](https://man7.org/linux/man-pages/man5/crontab.5.html).

```py
@online(cron="***/5 * * * ***")
def region_average(
    houses: DataFrame[
        House.city, House.rent_price
    ]
) -> DataFrame[Region.name, Region.average_price]:
    return houses.group_by(
        group={Region.name: House.city},
        agg={
            Region.average_price: op.mean(House.rent_price),
        },
    )
```

---

## Scheduling with arguments

Scheduled resolvers can also take arguments.
Chalk gives you control over which arguments should be
provided for each run of the schedule.

### All examples (default)

Chalk calls the resolver with
all unique argument names that could have called the function.
Consider a scheduled resolver that takes in
`Feature 1` and `Feature 2`:

<ScheduleSampling />

In this example, we compute two points to sample:
(1, 5) and (1, 7). These two values will be given
to the resolver on the specified cron schedule.

### Filtering examples

You can choose to filter down the set of all examples to run.
The `cron` keyword argument alternatively takes a `Cron` class.
By specifying a function from features to boolean,
you can tell Chalk which of the [default examples](#all-examples-default)
to run, and which to skip:

```py
def filter_examples(bank_id: User.bank_account.balance) -> bool:
    return balance > 100

@online(cron=Cron(schedule="1d", filter=filter_examples))
def fn(balance: User.bank_account.balance) -> ...:
```

The arguments to the filter function all need to be rooted in the
same entity as the features in the scheduled resolver,
but there is no requirement that the filter function take a subset
of the scheduled resolver's arguments:

```py
def filter_examples(status: User.bank_account.status) -> bool:
    return status == "opened"

@online(cron=Cron(schedule="1d", filter=filter_examples))
def fn(balance: User.bank_account.balance) -> ...:
```

### Custom examples

Finally, you can pick exactly the examples that you'd like to run.

```py
def pull_examples() -> DataFrame[User.id]:
    return DataFrame.read_csv(...)

@online(cron=Cron(schedule="1d", samples=pull_examples))
def fn(uid: User.id) -> ...:
```

In the above example, we provide all arguments to the resolver function.
However, you may also choose to provide only a subset of the arguments,
and Chalk will sample the other arguments.

```py
def pull_examples() -> DataFrame[User.id]:
    return DataFrame.read_csv(...)

@online(cron=Cron(schedule="1d", samples=pull_examples))
def fn(uid: User.id, balance: User.account.balance) -> ...:
```
