---
title: SQL Integration
description: Integrate with SQL-like sources.
---

import {
    Attribute,
    AttributeTable,
    SubAttribute,
    SubAttributeTable,
} from '@/components/AttributeTable'
import {RequestUrl} from '@/components/RequestUrl'

---

Chalk can ingest data using a SQL interface from any of integrations that support it. We also support incremental
offline ingestion of event tables.

## Basic queries

Chalk supports running SQL from files or from strings.

The examples on this page use our [PostgresQL data source](/docs/postgresql), but can be generalized to any of our other
SQL data sources. The full list Chalk supports can be found [in our API reference](/api-docs#BaseSQLSourceProtocol).

### SQL file resolvers
This is the preferred method of specifying a resolver.
Only a single `.chalk.sql` SQL file is needed. No Python required!

```sql get_user.chalk.sql
-- type: online
-- resolves: user
-- source: PG
select id, email, full_name from user_table
```

Here, we define an online resolver that returns some features for the `User` feature set
from the `user_table` table in the PostgreSQL source `PG`. The comments are `yaml`-parsed
to provide other metadata for Chalk to decide how to design the resolver. SQL file resolvers
can return multiple rows for aggregation operations, offline queries, and more.

It's also possible to use `SELECT *` in a SQL file resolver, but be careful!

```sql get_all_columns_users.chalk.sql
-- type: online
-- resolves: user
-- source: PG
select * from user_table
```

Implicitly, this query tries to align every scalar feature from the `User` feature set to a column name in
`user_table`. If a feature name is misnamed or absent from the table, you'll get a "missing columns" error.

To programmatically generate SQL file resolvers, check out [Generated SQL file
resolvers](/docs/sql#generated-sql-file-resolvers).

#### Parameterized feature inputs

Like other resolvers, SQL file resolvers can take features as input. In this example, we want our resolver to require
`EmailUser.id` as input:

```sql
-- type: online
-- resolves: user
-- source: PG
select id, email, full_name from user_table where id = ${email_user.id}
```

Use `${}` with snake case to reference the desired feature.

Use `${now}` in your query as a special argument representing the time of the query. For more details, see our
[Time](/docs/time#now-explicitly-time-dependent-resolvers) documentation.

### SQL strings

You can run SQL queries either as Python strings or from `.sql` files.

When the name of the column matches the name of the feature
with non-alphanumeric characters removed, the mapping from
column to feature is automatic.

```py
pg = PostgreSQLSource(name='PG')

@online
def get_user(uid: User.id) -> Features[User.full_name, User.email]:
    return (
        pg.query_string("select full_name, email from users where id=1")
        .first()
    )
```

`get_user`'s return type indicates that it expects features named `full_name` and `email`, which are returned as
columns from the SQL query.

If the column names don't align exactly, you can
include the parameter `fields` to specify the mapping from
the query to the fields.

```py
pg = PostgreSQLSource(name='PG')

@online
def get_user(uid: User.id) -> Features[User.full_name, User.email]:
    return (
        pg.query_string(
            "select name, email from users where id=1",
            fields=dict(name=User.full_name),
        )
        .first()
    )
```

Here, the `email` column of the query automatically aligns with the
expected `User.email` feature, but the `name` column of the query
is explicitly mapped to the `User.full_name` feature.

#### Parameterizing string queries

You can parameterize queries to pass variables.
Parameterize names with a colon,
and pass a dictionary from parameter name to parameter value:

```py
pg.query_string(
    query="select * from users where user_id=**:user_id**",
    args=dict(user_id="**uid123**"),
)
```

Use a backslash to escape any literal `:` characters you need to use in your query:

```py
pg.query_string(
    query="select * from users where user_id=**:user_id** and name=**'\:colon'**",
    args=dict(user_id="uid123"),
)
```


#### SQL string files

Instead of passing a string directly into your Python code, you can load the SQL content from a file.
you can use the `query_sql_file` function.

For example, here is a `query.sql` file containing the same query from above:

```sql query.sql
select * from users where user_id=:user_id
```

You can reference this file in a Python resolver, either
using the absolute path from the root of your project _or_
relative to the resolver's file.

For example, if the snippet below lived in the same directory
as `query.sql`, we could refer to it as follows:

```py
pg = PostgreSQLSource(name='PG')

@online
def get_user(uid: User.id) -> Features[User.full_name, User.email]:
    return (
        pg.query_sql_file(
            path="query.sql",
            args=dict(user_id=uid)
        )
        .first()
    )
```

Auto-mapping of column name to feature name also applies for
the `query_sql_file` method. Parameterization also works the same way.

## Configuration

The following are supported keys to be included in `.chalk.sql` file comments.

<AttributeTable title={"SQL File Resolvers"}>
<Attribute field='resolves' kind='str'>
    Describes the namespace to which the outputs belong.
    In the above example, <code className="dark:text-white">user.email</code>
    and <code className="dark:text-white">user.full_name</code> are the outputs.
</Attribute>
<Attribute field='source' kind='str'>
    Describes the database by <code className="dark:text-white">name</code>, as in the above example, or
    the type if there is only one database of that type. Thus, if you have
    one PostgreSQL source, you can also write <code className="dark:text-white">source: postgresql</code>.
</Attribute>
<Attribute field='type' kind='"online" | "offline" | "streaming" | null'>
    The type of resolver. If not specified, <code className="dark:text-white">online</code> is the default.
</Attribute>
<Attribute field='incremental' kind='dict[str, str]'>
    Parameters for incremental queries. For more information, see the <a
    href='/docs/sql#incremental-queries'>below</a> section on incremental queries.
    <SubAttributeTable title='incremental query parameters'>
        <SubAttribute field='mode' kind='"row" | "group" | "parameter"'>
            The incrementalization mode decides how to ingest new data.
        </SubAttribute>
        <SubAttribute field='lookback_period' kind='string'>
            The length of the window from the last observed row that Chalk will re-ingest, e.g. <code
            className="dark:text-white">1h</code>.
        </SubAttribute>
        <SubAttribute field='incremental_column' kind='string'>
            The timestamp column in the underlying table to use as the basis for incrementalization.
            Must be supplied in <code className="dark:text-white">row</code> and <code
            className="dark:text-white">group</code> modes.
        </SubAttribute>
        <SubAttribute field='incremental_timestamp' kind='"feature_time" | "resolver_execution_time"'>
            The timestamp used for timedelta calculations. Defaults to <code
            className="dark:text-white">feature_time</code>
        </SubAttribute>
    </SubAttributeTable>
</Attribute>
<Attribute field='count' kind='Literal[1, "one", "one_or_none"]'>
    Returns one. Equivalent to the common query finalizer <code className="dark:text-white">.one()</code>.
</Attribute>
<Attribute field='timeout' kind='Duration | null'>
    The maximum time to wait before timing out the query.
    See <a href="/docs/duration">Duration</a> for more details.
</Attribute>
<Attribute field='tags' kind='list[str] | null'>
    The user tags associated with this resolver. For online and offline resolvers.
</Attribute>
<Attribute field='environment' kind='list[str] | null'>
    The environments associated with this resolver.
</Attribute>
<Attribute field='cron' kind='str | null'>
    The schedule for a cron run, e.g. <code className="dark:text-white">1h</code>.
</Attribute>
<Attribute field='max_staleness' kind='str | null'>
    The max staleness for the resolver, e.g. <code className="dark:text-white">24h</code>.
</Attribute>
<Attribute field='owner' kind='str | null'>
    The owner of the resolver.
</Attribute>
<Attribute field='fields' kind='dict[str, str]'>
    An optional mapping from SQL column to Chalk feature.
    For example, with <code className="dark:text-white">SELECT name AS arbitrary_column_name</code>,
    we can map the <code className="dark:text-white">arbitrary_column_name</code> to a Chalk feature belonging to
    the
    namespace described by the <code className="dark:text-white">resolves</code> field with the mapping
    <code className="dark:text-white">arbitrary_column_name: chalk_feature_name</code>.
</Attribute>
</AttributeTable>

### Proper comment formatting

All comments must be inserted before the body of the sql query. Each comment line is parsed as
either a yaml-formatted line describing the resolver or a docstring. Below, the last comment
will appear as a docstring since it is not in `key:value` format.

```sql get_all_columns_users.chalk.sql
-- type: online
-- resolves: user
-- source: PG
-- This comment is not in yaml format, so it will be parsed as a docstring
select * from user_table
```

For field values that can be lists or dictionaries, such as `tags` or 'incremental` settings,
we can either enumerate the values inline or with an extra indentation.
Remember, if your values include a colon, you must use quotes around your value in order for
the line to be valid YAML format.

Both of the following formats are valid and equivalent.

```sql get_all_columns_users.chalk.sql
-- type: online
-- resolves: user
-- source: PG
-- tags: ["single:1", "double:2"]
select * from user_table
```

```sql get_all_columns_users.chalk.sql
-- type: online
-- resolves: user
-- source: PG
-- tags:
--    - single:1
--    - double:2
select * from user_table
```

### Streaming SQL File Resolvers

Chalk also supports SQL file streaming resolvers:

```sql get_store_features.chalk.sql
-- source: kafka
-- resolves: store_features
-- type: streaming
select store_id as id, sum(amount) as purchases
from topic
group by 1
```

### SQL Linting Configuration

If you are using [SQLFluff](https://www.sqlfluff.com/) or another SQL Linter,
you may need to set configurations to accept the variable pattern.
For SQLFluff, set the templater to `placeholder` and add the following to your config file.

```
# Allows sqlfluff to correctly parse
# ${arbitrary_placeholder_name}

[sqlfluff:templater:placeholder]
param_regex =\$\{[^}]*\}
1 = 'arbitrary_placeholder_name'
```

### Generated SQL file resolvers
You can programmatically generate SQL resolvers with [`make_sql_file_resolver`](/api-docs#make_sql_file_resolver). This
function is useful if you have many SQL tables and want to automate management of their resolvers.

```python
from chalk import make_sql_file_resolver
from chalk.sql import PostgreSQLSource

pg = PostgreSQLSource(name='PG')

definitions = [
    {
        resolver_name: "get_user_features",
        entity_name: "User",
        table: "user_table",
        pkey_column: "id",
        features: ["feature1", "feature2"],
    },
    ...
]

for definition in definitions:
    targets = ", ".join(definition.features)

    make_sql_file_resolver(
        name=definition.resolver_name,
        sql=f"select {definition.pkey_column}, {targets} from {definition.table}",
        source=pg, # "PG" is also acceptable
        resolves=definition.entity_name,
    )
```

`make_sql_file_resolver` adds this resolver to your registry as if it were a [SQL file
resolver](/docs/sql#sql-file-resolvers), but without creating the `.chalk.sql` file.

All SQL file resolvers require `source` and `resolves`. These values can be provided as SQL comments within your `sql`
value or as parameters. If comments and parameters are both provided, parameters will take precedence. This function
call is equivalent to the previous example:

```python
make_sql_file_resolver(
    name=definition.resolver_name,
    sql=f"""
        -- source: PG
        -- resolves: {definition.entity_name}
        select {definition.pkey_column}, {targets} from {definition.table}
    """,
)
```

## Incremental queries

### Why incremental?

The first time that a resolver with an incremental
query is executed, Chalk ingests all data from the source.
On subsequent runs of the resolver, Chalk only looks for
new rows in the table to ingest.
Using incremental queries limits the amount of data that
Chalk needs to ingest, lowering latency for updates
and reducing costs.

### When to use

Incremental queries are useful for ingesting immutable tables
or queries, like event tables or logs.
This type of data is frequently found in the offline
context, as it represents logs of real-time events.

### Using incremental queries (SQLAlchemy)

Imagine you have a login events table, where you keep track of
login attempts to your website.
You can ingest this table as follows:

```py
pg = PostgreSQLSource(name='PG')

@offline
def fn() -> DataFrame[Login.ts, Login.attemped_at, Login.user_id, Login.status]:
    return pg.query(
        Login(
            attempted_at=LoginHistorySQL.attempted_at,
            status=LoginHistorySQL.status,
            ts=LoginHistorySQL.created_at,
            user_id=LoginHistorySQL.user_id,
        )
    ).incremental()
```

Incremental queries need to map a [feature-time](/docs/features#feature-time)
feature (above as `Login.ts`). Using `.incremental` is equivalent to `.all`,
except Chalk can page over the underlying table using the column mapped to
the `feature_time` feature.

### Incremental queries with SQL File Resolvers

With Chalk SQL File resolvers, you can describe your incremental parameters
in typical YAML format.

```sql incremental_query.chalk.sql
-- type: offline
-- resolves: login
-- source: PG
-- incremental:
--   mode: row
--   lookback_period: 60m
--   incremental_column: attempted_at
select attempted_at, status, user_id from logins
```

### Using incremental queries (SQL-strings)

Chalk also supports incremental queries with raw SQL strings. If you use `.incremental` with string queries,
you must specify the `incremental_column` parameter.

```py
pg = PostgreSQLSource(name='PG')

@offline
def fn() -> DataFrame[Login.attemped_at, Login.user_id, Login.status]:
    return (
        pg.query_string("select attempted_at, status, user_id from logins")
          .incremental(incremental_column="attempted_at")
    )
```

### Handling late-arriving messages

If your underlying data source has "late arriving records", you may need to use the `lookback_period` argument to
`incremental`. When `lookback_period` is specified, Chalk subtracts the `lookback_period` from the
"maximum observed timestamp" that it uses as a lower-bound for ingestion.

Concretely, if your resolver body looks like this:

```py
db.query_string("SELECT * FROM payments")
    .incremental(incremental_column="updated_at", lookback_period="30m")
```

then Chalk will rewrite your SQL query to:

```sql
SELECT * FROM payments
WHERE updated_at >= (<previous_max_updated_at> - <lookback_period>)
```

This means that rows that arrive up to 30 minutes late will be properly ingested. The trade-off
is that Chalk will re-ingest some redundant data.

### Incrementalization modes

The default incrementalization mode for `.incremental` is `mode='row'`. Three modes are supported:

- **row**: Chalk ingests features from all rows whose `incremental_column` is newer than the previously observed max timestamp.
- **group**: Chalk ingests features from all _groups_ who are aggregating a _row_ that has been added or changed since the previously observed max timestamp.
- **parameter**: Chalk passes the `chalk_incremental_timestamp` value (including `lookback_period`) to your query, and leaves your query unchanged.

#### "Group" incremental mode

Group mode incremental ingestion is appropriate when you are aggregating rows in a table in order to compute
features.

For example, if you are running the following query:

```sql
SELECT
    business_id,
    SUM(amount) as sum_payments_amount,
    COUNT(*) as count_payments,
    max(updated_at) as ts
FROM payments
GROUP BY 1
```

to ingest features for this feature class:

```py
@features
class Business:
    id: int
    sum_payments_amount: float
    count_payments: int
    ts: FeatureTime
```

then you can specify the following resolver:

```py
@offline(...)
def resolve_aggregate_payment_features() -> DataFrame[Business]:
    query = """
        SELECT
            business_id,
            SUM(amount) as sum_payments_amount,
            COUNT(*) as count_payments,
            max(updated_at) as ts
        FROM payments
        GROUP BY 1
    """

    return db.query_string(query, fields={"business_id": Business.id}) \
                .incremental(incremental_column="updated_at", mode="group")
```

and Chalk will automatically rewrite your query into this form:

```sql
SELECT
    business_id,
    SUM(amount) as sum_payments_amount,
    COUNT(*) as count_payments,
    max(updated_at) as ts
FROM payments
WHERE business_id IN (
    SELECT DISTINCT(business_id) FROM payments
    WHERE updated_at >= :chalk_incremental_timestamp
)
GROUP BY 1
```

This means that if you have a payments table like this:

```
| id | business_id | amount | updated_at               |
| 1  | 1           | 10     | 2022-11-01T00:00:00.000Z |
| 2  | 1           | 5      | 2022-11-15T00:00:00.000Z |
| 3  | 2           | 7      | 2022-11-15T00:00:00.000Z |
| 4  | 3           | 17     | 2022-10-01T00:00:00.000Z |
```

and your query had previously run on `2022-11-07`, then Chalk would return the following aggregate values:

```
| business_id | sum_payments_amount | count_payments | ts                       |
| 1           | 15                  | 2              | 2022-11-01T00:00:00.000Z |
| 2           | 7                   | 1              | 2022-11-15T00:00:00.000Z |
```

Both business `1` and `2` are present, because they have at least one payment after `2022-11-07`.
Business `3` is excluded, since it has no payments that after `2022-11-07`.


#### "Parameter" incremental mode

In `parameter` incremental mode, Chalk leaves your query untouched. Chalk will simply pass the max incremental timestamp
to your query as a bind parameter named `chalk_incremental_timestamp`.

Concretely, if you write:

```py
@offline(...)
def parameter_incremental_mode_resolver() -> DataFrame[...]:
    return (
        db.query_string("SELECT * FROM payments WHERE updated_at >= :chalk_incremental_timestamp")
           .incremental(mode="parameter")
    )
```

Then Chalk will execute your query verbatim, and will keep track of the appropriate value for `chalk_incremental_timestamp`
between executions of your resolver.

### Incremental interaction with FeatureTime

When Chalk executes an incremental query, it has to update the "max timestamp" value that it will use as the
lower bound for the next query. By default, Chalk sets this value to the time at the _start_ of the query.

If your resolver returns a [FeatureTime](/docs/time) feature, Chalk will update the "max timestamp" value
to the "max" `FeatureTime` value that is returned instead. This allows you to control the incremental
behavior more precisely.



## Tagged SQL sources

Chalk supports applying [tags](/docs/resolver-tags) to sql sources. This allows you to define a single resolver that routes traffic
to multiple different backing databases depending on tags supplied at query time. This is useful for limiting
the blast-radius of traffic from different use-cases.

First, define a SQL source group:

```py
from chalk.sql import SQLSourceGroup, PostgreSQLSource

sql_group = SQLSourceGroup(
    name='primary_group',
    default=PostgreSQLSource(name="default_replica"),
    tagged_sources={
        'risk': PostgreSQLSource(name='risk_replica'),
        'analytics': PostgreSQLSource(name='analytics_replica'),
    }
)
```

Then, define a resolver that uses the group:

```py
@online
def users() -> DataFrame[User]:
    return sql_group.query_string("select id, name from users").all()
```

Then, when you submit queries, the query tags will control which datasource is used to execute the query:

```py
c = ChalkClient()

# This query uses the risk datasource
c.query(input={User.id: 1}, output=[User.name], tags=['risk'])

# This query uses the analytics datasource

c.query(input={User.id: 1}, output=[User.name], tags=['analytics'])

# This query uses the default datasource
c.query(input={User.id: 1}, output=[User.name])
```

## Additional query options

### SQLAlchemy
Chalk supports SQLAlchemy:

```py
pg = PostgreSQLSource(name='PG')

@online
def get_user(uid: User.id) -> Features[User.email, User.full_name]:
    return (
        pg.query(User(email=UserSQL.email, full_name=UserSQL.name))
        .filter_by(id=uid)
        .first()
    )
```

In the `.query(...)` call, you map the target columns of the
SQL statement to the feature namespace.
Here, we assign `User.email` to `UserSQL.email` and
`User.full_name` to `UserSQL.name`.
