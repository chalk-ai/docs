---
title: "Chalk Models"
metaDescription: Learn how to track, version, and run ML models in Chalk
description: Learn how to run and train ML models in Chalk
published: true
---

import { TipInfo } from '@/components/Tip'

---


With Chalk you can easily register and load machine learning models into your deployments. You can then run inference on these models. This guide covers how to integrate models into your Chalk applications, including loading existing models and running inference.

---

## Registering Models in Chalk

Chalk provides a simple way to register and manage machine learning models. You can register models from various frameworks like PyTorch, TensorFlow, and Scikit-learn.
Registering a model makes it available for inference in your Chalk deployments.

```python
from chalk.client import ChalkClient

client = ChalkClient()

input = InputSchema(
    type="tabular",
    features={
        "age": "int",
        "income": "float",
    }
)
output = OutputSchema(
    type="regression",
    shape=(1,),
    description="Predicted risk score"
)

client.register_model(
    name="RiskScoreModel",
    version=1,
    aliases=["v1.0"],
    model_path=["./risk_score_model.pth"],
    additional_files=[
        "./tokenizer.json"
    ],
    description="Model for predicting user risk scores",
    model_type="pytorch",
    model_format="pytorch",
    input=input,
    output=output,
    metadata={
        "framework": "pytorch",
        "training_date": "2025-07-29",
        "performance_metrics": {
            "accuracy": 0.95,
            "f1_score": 0.92
        }
    }
)
```

Models can also be registered from Python objects, such as Scikit-learn or PyTorch models. This allows you to register models directly from your
code without needing to save them to disk first.
```python
from chalk.client import ChalkClient
from sklearn.ensemble import RandomForestClassifier

client = ChalkClient()

rfc = RandomForestClassifier()

rfc.fit(X_train, y_train)


client.register_model(
    name="RiskScoreModel",
    version="1.0.0",
    aliases=["v1.0"],
    model=rfc,  # Directly pass the model object
    description="Model for predicting user risk scores",
    model_type="sklearn",
    model_format="pickle",
    metadata={
        "framework": "sklearn",
        "training_date": "2025-07-29",
        "performance_metrics": {
            "accuracy": 0.89,
            "precision": 0.91
        }
    }
)
```

### Registering from External Platforms

Models can also be registered from existing model artifacts in WandB or MLflow:

**From Weights & Biases:**
```python
client.model_from_wandb(
    name="RiskScoreModel",
    version="1.0.0",
    aliases=["v1.0"],
    wandb_run_id="abc123",
    wandb_project="risk-scoring",
    artifact_name="model:latest"
)
```

**From MLflow:**
```python
client.model_from_mlflow(
    name="RiskScoreModel",
    version="1.0.0",
    aliases=["v1.0"],
    mlflow_run_id="def456",
    mlflow_experiment_id="1",
    model_uri="models:/RiskScoreModel/1"
)
```

### Retrieving Model Information

```python
# Get model metadata
model = client.get_model(name="RiskScoreModel")
print(f"Latest version: {model.latest_version}")
print(f"Available versions: {model.versions}")

# Get specific version details
model_v1 = client.get_model(name="RiskScoreModel", version="1.0.0")
print(f"Performance: {model_v1.metadata['training_metrics']}")
```


---

## Including Models in Chalk Deployments

Use the `ModelVersion` class to include trained models in your Chalk deployment:

```python
from chalk.ml import ModelVersion
from chalk import functions as F

# Load a model into the deployment
risk_model = ModelVersion.from(
    name="RiskScoreModel",
    alias="2025-07-29",
)

# Reference the loaded model in inference
@features
class User:
    id: int
    age: int
    income: float
    risk_score: float = F.inference(risk_model, inputs=[
        _.age,
        10,
        _.income
    ])
```


## Running Inference on Models

Once models are registered and loaded into your deployment, you can run inference using several approaches:

```python
from chalk.ml import ModelVersion
from chalk import functions as F
from datetime import datetime

# Load a model into the deployment
risk_model_latest = ModelVersion.from(
    name="RiskScoreModel",
    alias="latest",
)

# Load a model into the deployment
risk_model_v1_0_0 = ModelVersion(
    name="RiskScoreModel",
    version=1,
)

# Load a model from a specific time
risk_model_asof = ModelVersion(
    name="RiskScoreModel",
    as_of=datetime(2025, 7, 29, 12, 0, 0)
)

# Reference the loaded model(s) in inference
@features
class User:
    id: int
    age: int
    income: float
    risk_score: float = feature(versions={
        1: feature(description="Latest model version", expression=F.inference(risk_model_latest, _.age, 10, _.income)),
        2: feature(description="Version 1.0.0", expression=F.inference(risk_model_v100, _.age, 10, _.income))
    })
```

---

## Custom Model Execution

<TipInfo>
    Model objects are fully accessible through their object's path for custom Python execution. This gives you complete control over how models are used in your resolvers.
</TipInfo>

For advanced use cases, you can load and use models directly in Python resolvers:

```python
from chalk import before_all, online
from chalk.features import features, _
from chalk.ml import ModelVersion

global model_executable
global fast_tokenizer

# Load the model version
risk_model_latest = ModelVersion.from(
    name="RiskScoreModel",
    alias="latest"
)

@before_all(resource_hint="gpu")
def load_model_artifacts():
    from transformers import PreTrainedTokenizerFast
    global model_executable, fast_tokenizer

    fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(
        risk_model_latest.additional_files["tokenizer.json"]
    )
    model_executable = risk_model_latest.load_model()

@online(resource_group="risk-model", resource_hint="gpu")
def embed_transaction_memo(memo: Transaction.memo) -> Transaction.memo_embedding:
    # Use the model directly in custom code
    tokens = fast_tokenizer.encode(memo, return_tensors="pt")

    return model_executable(tokens)
```

## Model Versioning and Management

Chalk provides flexible model versioning to help you manage model deployments:

### Version Selection

- **Specific version**: Use `version="1.0.0"` to load an exact version
- **Alias**: Use `alias="latest"` or `alias="production"` for dynamic references
- **Time-based**: Use `as_of=datetime(...)` to load the model version that was current at a specific time

### Model Aliases

Aliases allow you to reference models without hardcoding versions:

```python
# Set up aliases during registration
client.register_model(
    name="RiskScoreModel",
    version="2.1.0",
    aliases=["latest", "production"],
    # ... other parameters
)

# Use aliases in deployments
production_model = ModelVersion(
    name="RiskScoreModel",
    alias="production"
)
```

## Model Performance and Monitoring

When registering models, include performance metadata for tracking:

```python
client.register_model(
    name="RiskScoreModel",
    version="1.2.0",
    model=trained_model,
    metadata={
        "training_metrics": {
            "accuracy": 0.94,
            "precision": 0.91,
            "recall": 0.88,
            "f1_score": 0.89
        },
        "validation_metrics": {
            "accuracy": 0.92,
            "auc": 0.87
        },
        "dataset_info": {
            "training_samples": 50000,
            "validation_samples": 10000,
            "feature_count": 25
        }
    }
)
```

---

## Model Training in Chalk

Chalk provides distributed training capabilities that leverage your existing feature infrastructure.

### Single-Node Training

For smaller datasets that fit in memory, you can use single-node training:

```python
import os
import tempfile
import torch
import torch.nn as nn
import torch.nn.functional as F
import boto3
from chalk import DataFrame, offline, feature

class MyNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MyNeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        return self.fc2(F.relu(self.fc1(x)))

def transform(users: DataFrame[User]) -> torch.Tensor:
    feature_columns = [
        "user.num_transactions",
        "user.num_transactions_fraud",
        "user.avg_transaction_amount",
        "user.account_age_days",
        "user.percent_online_transactions",
    ]
    features_array = users[feature_columns].values
    return torch.tensor(features_array, dtype=torch.float32)

@feature
class Dataset:
    id: str
    users: DataFrame[User]
    labels: DataFrame[str]

@feature
class NNTrainer:
    dataset_id: Dataset.id
    dataset: Dataset
    model_artifact: str

@offline(resource_hint="gpu")
def train_model(
    dataset_id: Dataset.id,
    users: Dataset.users,
    labels: Dataset.labels,
) -> NNTrainer.model_artifact:
    lr = float(os.environ["lr"])
    batch_size = int(os.environ["batch_size"])
    num_epochs = int(os.environ["num_epochs"])

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    model = MyNeuralNetwork(input_size=5, hidden_size=10, output_size=2)
    model.to(device)

    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)

    features = transform(users).to(device)

    # Handle label conversion
    if isinstance(labels.iloc[0], str):
        label_mapping = {"not_fraud": 0, "fraud": 1}
        labels_tensor = torch.tensor(
            [label_mapping.get(label, 0) for label in labels],
            dtype=torch.long,
        )
    else:
        labels_tensor = torch.tensor(
            [int(label) for label in labels],
            dtype=torch.long
        )

    labels_tensor = labels_tensor.to(device)

    dataset = torch.utils.data.TensorDataset(features, labels_tensor)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
    )

    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_features, batch_labels in dataloader:
            outputs = model(batch_features)
            loss = loss_fn(outputs, batch_labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        if epoch % 10 == 0:
            avg_loss = total_loss / len(dataloader)
            print(f'Epoch [{epoch}/{num_epochs}], Average Loss: {avg_loss:.4f}')

    # Save model artifact
    artifact = {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "model_config": {
            "input_size": 5,
            "hidden_size": 10,
            "output_size": 2
        }
    }

    temp_dir = tempfile.mkdtemp()
    artifact_name = f"model_{dataset_id}.pt"
    artifact_path = os.path.join(temp_dir, artifact_name)

    torch.save(artifact, artifact_path)

    s3_bucket = os.environ["MODELS_BUCKET"]
    s3_key = f"models/{artifact_name}"

    s3_client = boto3.client('s3')
    s3_client.upload_file(artifact_path, s3_bucket, s3_key)
    s3_path = f"s3://{s3_bucket}/{s3_key}"

    os.remove(artifact_path)
    os.rmdir(temp_dir)

    return s3_path
```

### Distributed Training

For larger training datasets, data can be partitioned into blocks which can be processed in a stream using a sharded offline query:

```python
import os
import torch
import torch.nn as nn
import torch.nn.functional as F

from chalk import DataFrame as ChalkDataFrame
import chalk.train as chalk_train
from chalk.train import DataLoader

class MyNeuralNetwork(nn.Module):
    def __init__(self, input_size=, hidden_size, output_size):
        super(MyNeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        return self.fc2(F.relu(self.fc1(x)))

def transform(inputs: ChalkDataFrame) -> ChalkDataFrame[MyModel.inputs]:
  feature_columns = [
    "user.num_transactions",
    "user.num_transactions_fraud",
    "user.avg_transaction_amount",
    "user.account_age_days",
    "user.percent_online_transactions",
  ]
  features_array = inputs[feature_columns].values
  return torch.tensor(features_array, dtype=torch.float32)


class MyModel:
  features = [
    "user.num_transactions",
    "user.num_transactions_fraud",
    "user.avg_transaction_amount",
    "user.account_age_days",
    "user.percent_online_transactions",
  ]
  input_dtype = torch.float32
  output_dtype = torch.float32

  def load(self):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    self.model = MyNeuralNetwork(5, 10, 2)
    self.model.to(device)

  def predict(self, inputs: ChalkDataFrame):
      return self.model(transform(inputs))


def worker_function(worker_config: dict):
  lr = worker_config["lr"]
  batch_size = worker_config["batch_size"]
  num_epochs = worker_config["num_epochs"]

  # prepare model
  model = chalk_train.get_model()
  # could also make this automatic
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model.to(device)

  loss_fn = nn.MSELoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=lr)

  # prepare data
  train_dataset_shard = chalk_train.get_dataset_shard()
  dataloader = DataLoader(train_dataset_shard, batch_size=batch_size)

  # train on data
  for epoch in range(num_epochs):
    for batch in dataloader:
      output = model(transform(batch)) # revisit
      loss = loss_fn(output, batch["labels"])
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

    # log metrics + checkpoint
    chalk_train.log_metrics({"loss": loss.item()}, step=epoch)
    if (epoch % 5) == 0:
      checkpoint = {
        "epoch": epoch + 1,
        "model_state_dict": model.state_dict(),
        "optimize_state_dict": optimizer.state_dict(),
      }
      checkpoint_dir = chalk_train.get_checkpoint_dir()
      checkpoint_path = os.path.join(checkpoint_dir, f"epoch_{epoch+1}.pt")
      torch.save(checkpoint, checkpoint_path)
      chalk_train.log_artifact(checkpoint_path)

torch_trainer = TorchTrainer(
  worker_function=worker_function,
  worker_config={"num_epochs": 20, "lr": 0.01, "batch_size": 32},
  resource_config={"limit": {"cpu": 4, "memory": "16Gi", "gpu": 1}},
  # require one of `dataset` or `model`
  dataset="already_produced_dataset" # fetches dataset
  # model=MyModel # uses model input bindings to run offline query first
)

now_string = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
client = ChalkClient()
training_run = client.train_model(
  name=f"my_neural_network_exp_{now_string}",
  trainer=torch_trainer,
)

client.register_model_version(
  name="my_neural_network",
  aliases=["trial_2025_08_20"],
  uri=training_run.result.artifact,
)
```

## Supported Model Frameworks

Chalk supports models from various ML frameworks:

- **PyTorch**: `.pth`, `.pt` files and torch objects
- **TensorFlow**: SavedModel format and `.h5` files
- **Scikit-learn**: Pickled models and sklearn objects
- **XGBoost**: `.json`, `.model` files and xgboost objects
- **LightGBM**: `.txt` files and lightgbm objects
- **Custom**: Any Python object that implements prediction methods

## Best Practices

1. **Include metadata**: Always add performance metrics and training information
2. **Use aliases**: Set up `production`, `staging`, and `latest` aliases for easier deployment management
3. **Test before promoting**: Validate model performance in staging before updating production aliases
4. **Document dependencies**: Include all required packages and versions in the metadata