---
title: SQL Resolvers
description: What are sql resolvers.
---

SQL resolvers are a way to pull data from your data sources into your
features.

Below, is a simple example of a simple resolver that takes a user's email and
extracts the domain.

```
from chalk.features import features, online

@features
class User:
    id: str
    email: str
    email_domain: str

@online
def get_domain(email: User.email) -> User.email_domain:
    # type(email) == str
    return email.split('@')[1].lower()
```

In this section we'll go through [how to specify inputs](#inputs) to your python
resolvers, [how to specify outputs](#outputs), and [how to specify run conditons](#run-conditions).

## Inputs

## Outputs

To depend on features from a [Feature Set](/docs/features),
annotate your resolver arguments with your features as the types.

You can then use those arguments in the body of your resolver to
compute your [output features](/docs/resolver-outputs).

```py
from chalk.features import features, online

@features
class User:
    id: str
    email: str
    email_domain: str

@online
def get_domain(email: User.email) -> User.email_domain:
    # type(email) == str
    return email.split('@')[1].lower()
```

You can require multiple features in a resolver.
However, all feature dependencies in a single resolver
need to originate in the same root namespace:

<TipGood>Requiring features from the same root namespace</TipGood>

```py
@online
def fn(a: User.email, b: User.name) -> User.email_name_match:
    return ...
```

Here, we incorrectly request features from the root namespaces
of `Transfer` and `User`:

<TipBad>Requiring features from different root namespaces</TipBad>

```py
@online
def fn(a: User.email, b: Transfer.memo) -> Transfer.email_in_memo:
    return email in memo
```

If you want to require features from namespaces,
you can use [has-one](/docs/has-one) or
[has-many](/docs/has-many) relationships.

<TipGood>Requiring features from different root namespaces using a relationship</TipGood>

```py
@online
def fn(email: Transfer.user.email, memo: Transfer.memo) -> Transfer.email_in_memo:
    return email in memo
```

---

### Has-one dependencies

#### Scalar has-one

You can also require features joined to a
[Feature Set](/docs/features) through
[has-one](/docs/has-one) relationships.
For example, if users in your system have bank accounts,
and you wanted to compare the name on the user's bank account
to the user's name, you could require the user's name and
the account's title through the user:

```py
@online
def name_sim(title: User.account.title, name: User.full_name) -> ...
```

You can also require all scalars on the user's profile:

```py
@online
def fn(profile: User.profile) -> ...:
    profile.signup_date
```

Chalk will materialize all scalar features on the profile
before calling this function.
If you want to pull only a few features from the profile,
require each directly:

```py
@online
def fn(signup_date: User.profile.signup_date, age: User.profile.age) -> ...
```

#### Optional has-one

Has-one relationships can also be
[declared as optional](/docs/has-one#optional-relationships).
You may also require feature through optional relationships,
but the types for all of those optional features will become
optional. Consider the below example:

```py
@features
class Account:
    id: int
    user_id: int
    balance: float  # Non-optional balance

@features
class User:
    id: int
    # Optional relationship
    account: Account | None = has_one(lambda: Account.user_id == User.id)
    has_high_balance: bool

@online
def has_high_bal(balance: User.account.balance) -> User.has_high_balance:
    # Balance will be "float | None"
    if balance is None:
        return False
    return balance > 1000

```

The resolver in this example receives an optional `float`,
even though `balance` is not an optional field on `Account`.
The optional is added because the user may not have an account,
in which case the resolver will receive `None` for the balance.

#### Nested has-one

You can also traverse nested [has-one](/docs/has-one)
relationships in the same manner as
[requiring a single has-one](#has-one-dependencies).

Consider a schema where users have a feature set of profile information,
and the user's profile has an identity feature set, which in turn
has the age of the user's email.
You can require the email age feature as below:

```py
@online
def fn(email_age: User.profile.identity.email_age) -> ...
```

However, you cannot access nested relationships without
explicit asking for them.

<TipBad>Accessing a transitive relationship from a dependency.</TipBad>

```py
@online
def fn(acct: User.account) -> ...:
    acct.balance           # Ok
    acct.institution.name  # Error!
```

Instead, you can require the nested relationship directly
and access any of its scalar features.

<TipGood>Directly requiring the transitive relationship.</TipGood>

```py
@online
def fn(ins: User.account.institution, acct: User.account) -> ...:
    acct.balance  # Ok
    ins.name      # Ok
```

The semantics of
[optional has-one dependencies](#optional-has-one)
carry over to nested has-one dependencies.
If you traverse an optional relationship,
then all downstream attributes will become optional.

---

### Has-many dependencies

#### Scalar has-many

You can also require [has-many relationships](/docs/has-many)
as inputs to your resolver:

```py
@online
def fn(transfers: User.transfers) -> ...:
```

You receive a [Chalk DataFrame](/docs/dataframe),
which supports
[projections](/docs/dataframe#projections),
[filtering](/docs/dataframe#filters), and
[aggregations](/docs/dataframe#aggregations),
among other operations.

#### Projections

By default, Chalk will materialize all scalar features
on the `Transfer` feature set before calling your resolver.
As an optimization hint, you can specify which features from
the transfers that you'd like Chalk to materialize before calling
the function. For example, if there were expensive features
to compute on the transfer, you could scope the features
to only the set you need:

```py
@online
def fn(transfers: User.transfers[Transfer.amount, Transfer.memo]) -> ...:
    transfers[Transfer.amount].sum()      # Ok
    transfers[Transfer.from_institution]  # Error: filtered out above
```

The error above is surfaced statically
by our [editor plugin](/docs/editor-setup).


#### Filtering

You can apply filters to the has-many inputs of resolvers:

```py
@online
def fn(transfers: User.transfers[Transfer.amount > 100]) -> ...:
```

[Filters](/docs/dataframe#filters) can be composed with
[projections](/docs/dataframe#projections) following the semantics
of the Chalk `DataFrame`.

```py
@online
def fn(transfers: User.transfers[Transfer.amount > 100, Transfer.memo]) -> ...:
```

Filters can also be used on features through has-one relationships.

```py
@online
def fn(transfers: User.transfers[Transfer.amount, Transfer.bank.location == "USA"]) -> ...:
```

More on projection and filtering [here](/docs/dataframe#composing-projections-and-filters)

#### Has-many through has-one

Has-many relationships can be required through has-one relationships:

```py
@online
def fn(transfers: User.account.transfers) -> ...:
```

As with [scalar has-many dependencies](#scalar-has-many),
you can scope down the scalar features on the transfer
to only those required:

```py
@online
def fn(transfers: User.account.transfers[Transfer.amount]) -> ...:
    transfers[Transfer.amount].sum()      # Ok
    transfers[Transfer.from_institution]  # Error: filtered out above
```

#### Has-many through optional-has-one

If the has-one relationship that you're traversing is
[optional](/docs/has-one#optional-relationships),
then the `transfers` argument in the example above will
either be `None` or a [Chalk DataFrame](/docs/dataframe).

#### Has-one through has-many

You can also select columns through nested has-one relationships that would not normally materialize.

```py
@online
def fn(transfers: User.transfers[Transfer.amount, Transfer.bank.name]) -> ...:
```

In the above example, if there was a has-one relationship between `Transfer` and `Bank`,
we can fetch any scalar feature from `Bank` as well for the `DataFrame`.
Note that the `Bank.name` column would _not_ materialize with the simple `transfers: User.transfers` typing,
since this typing only materializes scalar features in the root namespace.

#### Has-many through has-many

Has-many relationships can be required through other has-many relationships.

For example, consider the following feature definitions for `User`, `Account`, and `Transaction`,
where a user can have many accounts, each with many transactions.

```py
from chalk.features import features, has_many, DataFrame


@features
class Transaction:
    id: str
    account_id: str
    amount: float


@features
class Account:
    id: str
    user_id: str
    transactions: DataFrame[Transaction] = has_many(
        lambda: Transaction.account_id == Account.id
    )


@features
class User:
    id: str
    total_spent: float
    accounts: DataFrame[Account] = has_many(lambda: Account.user_id == User.id)
```

We can resolve the `total_spent` feature on `User` by computing the sum of transaction
amounts across all of a user's accounts, as shown below.

```python
@online
def get_total_spent(
    txns: User.accounts.transactions[Transaction.amount]
) -> User.total_spent:
    return txns.sum()
```

### Time Filtering

Window functions are helpful for creating features that describe
recent behavior.
With window functions, you can implement features such as moving
averages or counts of events in a window.

Window functions are computed on a [DataFrame](/docs/dataframe).
A [has-many](/docs/has-many) feature creates a [DataFrame](/docs/dataframe)
on a feature set, which you can filter with the special functions
`before` and `after`.
These functions filter your data relative to the
current time in context. This time could be in the past
if you're using an offline resolver.
Using window functions ensures that you maintain
[point-in-time correctness](/docs/temporal-consistency).

Chalk supplies two functions, `before` and `after`,
which take many keyword arguments describing the time
relative to the present:

```py
# Keep where over three years and two days old
before(years_ago=3, days_ago=2)

# Keep where under 5:10 minutes old
after(minutes_ago=5, seconds_ago=10)
```

You can use these operators to filter out rows of a `DataFrame`:

#### After

To compute the number of transfers a user made in the
last seven days, you can use the `after(...)` function:

```py
from chalk.features import after, ...

@online
def fn(transfers: User.transfers[after(days_ago=7)]) -> ...:
    return transfers.count()
```

#### Before

Alternatively, if you wanted to compute the number of transfers
a user made more than seven days ago, you would write:

```py
from chalk.features import before, ...

@online
def fn(transfers: User.transfers[before(days_ago=7)]) -> ...:
    return transfers.count()
```

#### Combining before and after

You can also combine `before` and `after`.
In this example, we return the transfers made over a week ago,
but less than two weeks ago.
These filters can also be combined with other filters and projections
as described [here](/docs/dataframe#composing-projections-and-filters).

```py
from chalk.features import before, after, ...

@online
def fn(transfers: User.transfers[after(days_ago=14), before(days_ago=7)]) -> ...:
    return transfers.count()
```

### Now: Explicitly time-dependent resolvers

Chalk supports resolvers that are explicitly time-dependent. This is useful for performing backfills which
compute values that depend on timestamps that are semantically similar to `datetime.now()`.

You can express time-dependency by declaring a dependency on a special feature called `Now`,
which gets converted into a `datetime` within resolvers:

```py
@online
from chalk.features import Now

def get_age_in_years(birthday: User.birthday, now: Now) -> User.age_in_years:
    return (now - birthday).years
```

Note, in online queries, (i.e. with `ChalkClient().query`), `Now` is `datetime.now()` if the `now` parameter is unused.
In offline query contexts, `now` will be set to the appropriate `input_time` value for the calculation.
This allows you to backfill a feature for a single entity at many different historical time points:

```py
ChalkClient().offline_query(input={User.id: [1,1,1]}, output=[User.age_in_years], input_times=[
    datetime.now() - timedelta(days=365*10),
    datetime.now() - timedelta(days=365),
    datetime.now() - timedelta(days=0),
])

## output:

# | id | age_in_years |
# | 1  | <age> - 10   |
# | 1  | <age> - 1    |
# | 1  | <age> - 0    |
```

`Now` can be used in DataFrame resolvers as well in order to compute bulk values:

```py
@online
def batch_get_age_in_years(df: DataFrame[User.id, User.birthday, Now]) -> DataFrame[User.id, User.age_in_years]:
    return (
        df.to_polars()
            .select(
                pl.col(User.id),
                pl.col(str(User.birthday) - pl.col(str(Now))).alias(str(User.age_in_years))
            )
    )
```

## Outputs

SQL Resolvers specify the type of their outputs in comments:

Resolvers declare the features that they resolve through a
[Python type annotation](https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html#functions)
on the return value of the function.

## Scalar output

### Single output

To return a single feature from a resolver,
set the return type annotation to the feature you
want to resolve:

```py
from chalk.features import features

@features
class User:
    id: int
    name: str
    employer: str

@online
def resolve(u: User.id) -> User.name:
    return "Jennifer Doudna"
```

Equivalently, you can wrap the return value in the `User` class:

```diff-py
@online
def resolve(u: User.id) -> User.name:
-   return "Jennifer Doudna"
+   return User(name="Jennifer Doudna")
```

### Multiple outputs

To return multiple features, return an
[instance of the feature class](/docs/features#constructing-feature-classes).
In the type signature, specify
the `Features[...]` class, parameterized
by the features that you pass to the feature class.

```diff-py
@online
def resolve(u: User.id) -> Features[User.name, User.employer]:
    return User(
        name="Jennifer Doudna",
        employer="University of California, Berkeley"
    )
```

You only need to pass
[a subset of the features](/docs/features#constructing-feature-classes)
to the constructor for the feature class.

The [editor plugin](/docs/editor-setup) will check
that the type annotation you assign to the resolver
matches subset of features passed to the constructor
of the feature set.

### All features

To return all features of a class,
use `Features[...]` around the feature class.

```py
@online
def get_user(u: User.id) -> Features[User]:
    return User(
        name="Jennifer Doudna",
        employer="University of California, Berkeley"
    )
```

If your resolver takes input features, those features are not
considered as part of the output features.

Note that the `id` feature is not returned from the function.

This definition is equivalent to:

```diff-py
@online
- def get_user(u: User.id) -> Features[User]:
+ def get_user(u: User.id) -> Features[User.name, User.employer]:
    return User(
        name="Jennifer Doudna",
        employer="University of California, Berkeley"
    )
```

However, you may want to return _almost_ all features of a class.
Writing out all the features can be tedious and error-prone.
You can subtract features from a feature class
using the `-` operator:

```py
from chalk.features import Features, ...

@online
def get_all_users(id: User.id) -> Features[User] - User.name:
    return User(employer="University of California, Berkeley")
```

Here, both the `id` feature and the `name` feature are not returned,
which leaves only the `employer` feature.


---

## DataFrame output

You can also output many instances of a feature set from a resolver
by specifying a [DataFrame](/docs/dataframe) as the return type of
the function:

```py
@offline
def get_events() -> DataFrame[Transfer.uuid, Transfer.amount, Transfer.ts]:
    return DataFrame.read_csv(...)
```

For more info on how to load batch data,
see the [Data Sources](/docs/sql) sections.
`DataFrame`-returning resolvers don't need inputs.


### All features

To return all features of a class in a DataFrame,
use `DataFrame[...]` class around the feature class:

```py
@online
def get_all_users() -> DataFrame[User]:
    return DataFrame([
        User(
            name="Jennifer Doudna",
            employer="University of California, Berkeley"
        )
    ])
```

### Other DataFrame-returning resolvers

Imagine a scalar feature you'd like to backfill over many thousands of pkeys and historical times.
`DataFrame`-returning resolvers can dramatically reduce the computation time due to its vectorized handling.

```py
@offline
def get_new_feature_as_dataframe(
    df: DataFrame[Transaction.id, ...]
) -> DataFrame[Transaction.id, Transaction.new_feature]:
```

The above resolver runs faster on a thousand rows
than the equivalent scalar resolver ran a thousand times.

Chalk also supports relationship-returning resolvers that enable users to
return a `DataFrame` belonging to a has-many relationship.

```py
@offline
def relationship_returning_resolver(
    df: User.transactions[Transaction.id, Transaction.amount, Transaction.description],
    user_type: User.type
) -> User.transactions[Transaction.id, Transaction.transaction_type]:
```

Just make sure that the return `DataFrame`s do not have duplicate rows.
That means no two rows should have the same primary key, or primary key & timestamp combinations if the
feature time is also returned.

### Overriding Output Time

By default, features are timestamped with the execution time of their resolver. However, you can override this behavior by providing timestamps from your data source This functionality can be helpful when working with an event store or timestamped API.
The first step is to [add a FeatureTime or `ts` feature to your feature set](/docs/feature-types#feature-time).

You can then run resolver which sets the feature time of your

## Run Conditions

Both offline and online resolvers [support scheduled runs](/docs/resolver-cron).
However, you're likely to consider scheduling more frequently with offline resolvers.
Every offline resolver pulls in data from your underlying source to
Chalk's feature store on a schedule.
Chalk will determine the frequency at which to poll your data sources,
or you can choose to provide a custom [duration](/docs/duration) on which
to pull the data.

Online resolvers can also be scheduled, although they don't _necessarily_
run on a schedule as offline resolvers do.

Both online and offline scheduled resolvers may take arguments,
and you have
[control over the sets of arguments](/docs/resolver-cron#filtering-examples)
to run.


---

## Interaction

### Online-to-Offline

After an online resolver runs, its values are copied into the [offline store](#storage).
When you [query the offline store](/docs/training-client),
you will receive data from both records of online runs and offline-specific resolvers.
Which data you receive depends on which data was closest to the point-in-time
that you queried.
For more information, see [temporal consistency](/docs/temporal-consistency).

### Offline-to-online

In contrast, data from the offline environment _does not_
reach the online store by default.
However, you can choose to ETL the data from an offline
resolver into the online store.
This can be helpful, for example,
when you tolerate stale data in online inference and
have a data source in the offline environment that doesn't
have a direct replacement in the online environment.
More details are provided in the section [Reverse ETL](/docs/reverse-etl).

## Summary

| Online query                                                                                                                  | Offline query                                                                                                                                                                                                                                |
|-------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Runs only <code className="whitespace-nowrap before:content-none text-pink-400 after:content-none"> @online </code> resolvers | Runs both <code className="whitespace-nowrap before:content-none text-pink-400 after:content-none"> @online </code> and <code className="whitespace-nowrap before:content-none text-pink-400 after:content-none"> @offline </code> resolvers |
| Returns one row of data about one entity                                                                                      | Returns a dataframe of many rows of historical data corresponding to multiple entities point-in-time                                                                                                                                         |
| Designed to return data immediately in milliseconds                                                                           | Blocks until computation is complete, not designed for millisecond-level computation                                                                                                                                                         |
| Queries the online store, which caches recent data from online queries for quick retrieval                                    | Queries the offline store (Timescale), which stores all data from both online and offline queries                                                                                                                                            |
| Writes output data to online store database and offline store database                                                        | Writes output to offline store database and a parquet file containing results to cloud storage. Only writes to online store if specified.                                                                                                    |


Tags allow you to scope requests within an environment.
Both tags and environment need to match for a resolver to
be a candidate to execute.

You might consider using tags, for example, to change out
whether you want to use a sandbox environment for a vendor,
or to bypass the vendor and return constant values in a
`staging` environment.

## Restricting Resolver Execution By Environment

Environments are used to trigger behavior in different deployments
such as staging, production, and local development.
For example, you may wish to interact with a vendor via an API call
in the `production` environment, and opt to return a constant value
in a `staging` environment.

## Specifying environments

You can choose to scope resolvers to a restricted set of environments.
Resolvers optionally take a keyword argument named `environment`
that can take one of three types:

- **Unassigned (default)** - The resolver will be a candidate to run in _every_ environment.
- **String value** - The resolver will run _only_ in this environment.
- **List of strings** - The resolver will run in _any_ of the specified environments and no other environments.

## Example

Say your fraud models needed to interact with a fraud vendor that you wanted
to mock out in staging. We can scope the environments as follows:

```python
@online(environment="**production**")
def fraud_score_prod(email: User.email, phone: User.phone) -> User.fraud_score:
    return api_vendor.fraud_score(email)

@online(environment=["**staging**", "**dev**"])
def fraud_score_staging(email: User.email) -> User.fraud_score:
    if email == "fraud_user@chalk.ai":
        return 10
    return 90
```

Resolvers in different environments don't need to take the same arguments.
In the above example, the production version of the resolver takes a phone
number and an email, while the staging version of the resolver takes only
the email.

## Restricting Resolver Execution Conditions With Tags

Tags can be either a string value, or a key-value pair.
As a best practice
(and fitting with recommendations from other services like
[Datadog](https://docs.datadoghq.com/getting_started/tagging/#unified-service-tagging)),
we recommend using key-value pairs, but the choice is yours.

There are two ways to specify tags on resolvers:

- **<code className="before:content-none after:content-none">"key"</code>** -
As a single scalar string.
- **<code className="before:content-none after:content-none">"key:value"</code>** -
As a string value representing a key-value pair.

Resolvers take one or many tags, all of which need to match for the
resolver to run. For example, you may want to test with
a sandboxed vendor, and also be able to set a constant value
for a particular feature.

```py
@online(tags=["**api:mock**", "**fraud**"])
def simulate_fraud(id: User.id) -> User.fraud_score:
    return 100

@online(tags="**api:mock**")
def simulate_no_fraud(id: User.id) -> User.fraud_score:
    return 10

@online(tags="**api:sandbox**")
def sandbox_score(
    name: User.name,
    email: User.email,
) -> User.fraud_score:
    return sandbox.fraud_score(name, email).score

@online
def real_score(
    name: User.name,
    email: User.email,
) -> Features[User.fraud_score, User.fraud_tags]:
    r = prod.fraud_score(name, email).score
    return User(fraud_score=r.score, fraud_tags=r.tags)
```

In the above example, the resolver that is chosen to compute
the `User.fraud_score` feature will depend on the tags provided
at query time. The table below shows which resolver will be
chosen for a given set of tags.

| Query tags                                                              | Resolver                                                             |
| ----------------------------------------------------------------------- | -------------------------------------------------------------------- |
| <span className="font-mono text-accent-teal"> [api:mock, fraud] </span> | <span className="font-mono text-pink-500"> simulate_fraud </span>    |
| <span className="font-mono text-accent-teal"> api:mock </span>          | <span className="font-mono text-pink-500"> simulate_no_fraud </span> |
| <span className="font-mono text-accent-teal"> api:sandbox </span>       | <span className="font-mono text-pink-500"> sandbox_score </span>     |
| <span className="font-mono text-accent-teal"> &lt;otherwise&gt; </span>       | <span className="font-mono text-pink-500"> real_score </span>        |

Note that these resolvers don't need to take the same set of inputs,
and don't need to return the same types.

## When tagged resolvers run

Like [Environments](/docs/resolver-environments), tags control when resolvers run
based on the
[Online Context](/docs/query-basics) or [Training Context](/docs/training-client)
matching the tags provided to the resolver decorator.
Resolvers optionally take a keyword argument named `tags`
that can take one of three types:

- **Unassigned (default)** - The resolver will be a candidate to run for _every_ set of tags.
- **String value** - The resolver will run _only_ if this tag is provided.
- **List of strings** - The resolver will run _only_ if _all_ of the specified tags match.

If your resolver is tagged only by a key (not a key-value pair),
and the request context contains a key-value pair such that the resolver's
tag (a key only) matches they key of a key-value pair in the context,
the resolver will be eligible to run. For example:

| Resolver Tag                                                                 | Request Context                                                           | Matches? |
| ---------------------------------------------------------------------------- | ------------------------------------------------------------------------- | -------- |
| <span className="font-mono text-accent-teal"> api </span>                    | <span className="font-mono text-pink-500"> api </span>                    | Yes      |
| <span className="font-mono text-accent-teal"> api </span>                    | <span className="font-mono text-pink-500"> api:live </span>               | Yes      |
| <span className="font-mono text-accent-teal"> [api:live, mock-phone] </span> | <span className="font-mono text-pink-500"> [api:live, mock-phone] </span> | Yes      |
| <span className="font-mono text-accent-teal"> [api:live, mock-phone] </span> | <span className="font-mono text-pink-500"> api:live </span>               | No       |
| <span className="font-mono text-accent-teal"> api:live </span>               | <span className="font-mono text-pink-500"> api </span>                    | No       |
| <span className="font-mono text-accent-teal"> api:fixture </span>            | <span className="font-mono text-pink-500"> api:live </span>               | No       |

## Example

Frequently, you'll want to combine tags and [Environments](/docs/resolver-environments),
as below.
This span uses a constant value in staging when the tag `api` takes the value `fixture`,
uses the sandboxed fraud vendor in staging when the tag `api` takes the value `live`,
and uses the production fraud vendor in production.

```python
@online(environment="staging", tags="**api:fixture**")
def fraud_score_fixture(email: UserFeatures.email) -> UserFeatures.fraud_score:
    if email == "elliot@chalk.ai":
        return 100
    return 50

@online(environment="staging", tags="**api:live**")
def fraud_score_sandbox(email: UserFeatures.email) -> UserFeatures.fraud_score:
    return api_vendor_sandbox.fraud_score(email, profile="dev")

@online(environment="production")
def fraud_score_prod(email: UserFeatures.email) -> UserFeatures.fraud_score:
    return api_vendor_live.fraud_score(email)
```
