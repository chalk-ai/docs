---
title: Stream Sources
description: Consuming Kafka streams.
published: true
---

import { Attribute, AttributeTable } from '@/components/AttributeTable'

---

Chalk supports Kafka, Kinesis, and PubSub as [streaming sources](/docs/streams)
for `@stream` resolvers.

---

## Configuration options

You can configure the Kafka-, Kinesis-, or PubSub-specific options
either explicitly or through the Chalk dashboard.

### Named integration

If configuring through the dashboard, your Kafka/Kinesis integration
must be given a name. You can then reference this name in your
stream configuration.

<AttributeTable title={'Options'}>
  <Attribute field={'name'} kind={'string'}>
    The name of the integration as configured in your Chalk dashboard.
  </Attribute>
</AttributeTable>

### Explicit configuration

You can instead choose to configure the source in your code directly for the following source types.

<AttributeTable title={"Kafka Options"}>

<Attribute field={'bootstrap_server'} kind={'string | string[]'}>
  The Kafka broker's host name without the security protocol. You can specify multiple brokers by
  passing a list of strings. Example: <code>"localhost:9092"</code> or <code>["localhost:9092", "localhost:9093"]</code>
</Attribute>

<Attribute field={'topic'} kind={'string | string[]'}>
  The topic or topics to subscribe to.
</Attribute>

<Attribute field={'ssl_keystore_location'} kind={'string?'}>
  An S3 or GCS URI that points to the keystore file that should be used for brokers. You must
  configure the appropriate AWS or GCP integration in order for Chalk to be able to access these
  files.
</Attribute>

<Attribute field={'client_id_prefix'} kind={'string?'}>
  Optionally, you may specify a prefix that will be used for client ids generated by <code>@stream</code>
  resolvers that consume this source.
</Attribute>

<Attribute field={'group_id_prefix'} kind={'string?'}>
  Optionally, you may specify a prefix that will be used for group ids generated by <code>@stream</code>
  resolvers that consume this source.
</Attribute>

<Attribute
  field={'security_protocol'}
  kind={<span>'PLAINTEXT' | 'SSL' | 'SASL_PLAINTEXT' | 'SASL_SSL'</span>}
>
  Security protocol passed directly to Kafka. Defaults to 'PLAINTEXT'.
</Attribute>

<Attribute
  field={'sasl_mechanism'}
  kind={
    <span>
      <br />
      'PLAIN' | 'GSSAPI' | 'SCRAM-SHA-256' | 'SCRAM-SHA-512' | 'OAUTHBEARER'
    </span>
  }
>
  Authentication mechanism when security_protocol is configured for SASL_PLAINTEXT or SASL_SSL.
  Defaults to 'PLAIN'.
</Attribute>

<Attribute field={'sasl_username'} kind={'string?'}>
  Username for SASL PLAIN, SCRAM-SHA-256, or SCRAM-SHA-512 authentication. Defaults to null.
</Attribute>

<Attribute field={'sasl_password'} kind={'string?'}>
  Password for SASL PLAIN, SCRAM-SHA-256, or SCRAM-SHA-512 authentication. Defaults to null.
</Attribute>
</AttributeTable>


<AttributeTable title={"Kinesis Options"}>

<Attribute field={'stream_name'} kind={'string?'}>
    The name of your stream. Either this or the stream_arn must be specified.
</Attribute>

<Attribute field={'stream_arn'} kind={'string?'}>
    The ARN of your stream. Either this or the stream_name must be specified.
</Attribute>

<Attribute field={'region_name'} kind={'string'}>
    The AWS region, e.g. "us-east-2".
</Attribute>

<Attribute field={'aws_access_key_id'} kind={'string?'}>
    The AWS access key id credential, if not already set in the environment.
</Attribute>

<Attribute field={'aws_secret_access_key'} kind={'string?'}>
    The AWS secret access key credential, if not already set in the environment.
</Attribute>

<Attribute field={'aws_session_token'} kind={'string?'}>
    The AWS session token credential, if not already set in the environment.
</Attribute>

<Attribute field={'endpoint_url'} kind={'string?'}>
    An optional endpoint to hit the Kinesis server.
</Attribute>

<Attribute field={'consumer_role_arn'} kind={'string?'}>
    An optional role ARN for the consumer to assume.
</Attribute>
</AttributeTable>

<AttributeTable title={"PubSub Options"}>

<Attribute field={'project_id'} kind={'string?'}>
    The project id that your pubsub source resides in.
</Attribute>

<Attribute field={'subscription_id'} kind={'string?'}>
    The subscription id of your pubsub source.
</Attribute>
</AttributeTable>

## Example

```py
from pydantic import BaseModel
from chalk import stream, feature
from chalk.streams import KafkaSource
from chalk.features import features, Features

@features
class User:
    id: str
    favorite_color: str = feature(max_staleness='30m')

class UserUpdateBody(BaseModel):
    user_id: str
    favorite_color: str

kafka_source = KafkaSource(name="user_favorite_color_updates")

@stream(source=kafka_source)
def fn(message: UserUpdateBody) -> Features[User.uid, User.favorite_color]:
    return User(
        id=message.value.user_id,
        favorite_color=message.value.favorite_color
    )
```
