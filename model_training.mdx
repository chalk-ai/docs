---
title: "Model Training"
metaDescription: Learn how to train ML models in Chalk
description: Learn how to train ML models in Chalk
published: true
---

import { TipInfo } from '@/components/Tip'

---

Chalk provides single node and distributed training capabilities that leverage your existing feature infrastructure
and generate [Model Artifacts](/docs/model_registry). These Model Artifacts can be registered into the [Model Registry](/docs/model_registry)

## Training Overview

Chalk supports two primary training approaches:

- **Single-Node Training**: For smaller datasets that fit in memory on a single machine
- **Distributed Training**: For large-scale datasets that require distributed processing across multiple workers

Both approaches integrate seamlessly with your existing Chalk feature infrastructure and automatically register trained models in the [Model Registry](/docs/model_registry)
and can be run from a Jupyter notebook or as part of a CI/CD pipeline. In this tutorial, we cover both training methods run through a jupyter notebook:

---

## Single-Node Training

Single-node training is ideal for smaller datasets that can fit comfortably in memory. This approach is simpler to set up and debug, making it perfect for experimentation and smaller production workloads.

### Basic Training Setup

Here's a complete example of single-node training with PyTorch.
Define your training code using `chalk_train` and `chalk_logger` for logging,
checkpointing, and dataset loading.

```python
import torch
import torch.nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.feature_extraction.text import TfidfVectorizer

import chalk.ml.train as chalk_train
from chalk import chalk_logger


class SpamClassifier(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.fc = nn.Sequential(nn.Linear(input_size, 32), nn.ReLU(), nn.Linear(32, 2))

    def forward(self, x):
        return self.fc(x)


# Define the preprocessing and data loading code
def preprocess_data(dataset_name: str, config: dict):
    # Use Chalk Train to Load Dataset for Training
    df = chalk_train.load_dataset(dataset_name=dataset_name).to_pandas()

    data = df[['sms_spam.content']]
    labels = df['sms_spam.label'].map({'ham': 1, 'spam': 0}).values

    tfidf = TfidfVectorizer(max_features=50)

    X = torch.tensor(tfidf.fit_transform(data).toarray(), dtype=torch.float32)
    y = torch.tensor(labels, dtype=torch.long)

    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=config["batch_size"], shuffle=True)
    return dataloader, X.shape[1]


# Define the training code for the model
def train(dataset_name: str, config: dict):
    dataloader, input_size = preprocess_data(dataset_name=dataset_name, config=config)
    model = SpamClassifier(input_size)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config["lr"])

    for epoch in range(config["num_epochs"]):
        total_loss = 0
        correct = 0
        total = 0

        for batch_X, batch_y in dataloader:
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == batch_y).sum().item()
            total += batch_y.size(0)

        if epoch % 5 == 0:
            accuracy = correct / total

            chalk_logger.info(f'Epoch {epoch}: Loss={total_loss/len(dataloader):.3f}, Acc={accuracy:.3f}')

            chalk_train.checkpoint(
                model,
                metadata=dict(
                    accuracy=accuracy,
                    epoch=epoch,
                )
            )
```

Once your train model is defined, you can run a training job using the client.train_model method:

```python
# Create a training run
training_run = client.train_model(
    train_fn=train,
    dataset_name=dataset.dataset_name,
    model_name="spam_model",
    config=dict(
        lr=0.01,
        num_epochs=50,
        batch_size=32
    ),
    resources=ResourceRequests(
        cpu=15,
        memory="15Gi",
        resource_group="gpu"
    )
)
```

If you've defined a resource group with GPU access, you can also leverage this for training by specifying the appropriate resource group.

## Distributed Training

For larger training datasets that don't fit in memory or require distributed processing, Chalk provides distributed training capabilities. Data is automatically partitioned into blocks and processed across multiple workers.

### Distributed Training Architecture

Distributed training in Chalk uses the following components:

- **Model Classes**: Define your model structure, features, and data types
- **Worker Functions**: Handle the actual training logic on each distributed worker
- **TorchTrainer**: Orchestrates distributed training across multiple workers
- **Data Sharding**: Automatic partitioning of datasets across workers

### Setting Up Distributed Training

```python
import os
import torch
import torch.nn as nn
import torch.nn.functional as F

from chalk import DataFrame as ChalkDataFrame
from chalk.typing import InputSchema, OutputSchema
import chalk.train as chalk_train
from chalk.train import DataLoader

class MyNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MyNeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        return self.fc2(F.relu(self.fc1(x)))

def transform(inputs: ChalkDataFrame) -> torch.Tensor:
  feature_columns = [
    "user.num_transactions",
    "user.num_transactions_fraud",
    "user.avg_transaction_amount",
    "user.account_age_days",
    "user.percent_online_transactions",
  ]
  features_array = inputs[feature_columns].values
  return torch.tensor(features_array, dtype=torch.float32)


class MyModel:
  features = [
    "user.num_transactions",
    "user.num_transactions_fraud",
    "user.avg_transaction_amount",
    "user.account_age_days",
    "user.percent_online_transactions",
  ]
  input_dtype = torch.float32
  output_dtype = torch.float32

  def load(self) -> None:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    self.model = MyNeuralNetwork(5, 10, 2)
    self.model.to(device)

  def predict(self, inputs: ChalkDataFrame) -> torch.Tensor:
      return self.model(transform(inputs))
```

### Defining the Worker Function

The worker function contains the core training logic that runs on each distributed worker:

```python
def worker_function(worker_config: dict):
  lr = worker_config["lr"]
  batch_size = worker_config["batch_size"]
  num_epochs = worker_config["num_epochs"]

  # prepare model
  model = chalk_train.get_model()
  # could also make this automatic
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model.to(device)

  loss_fn = nn.MSELoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=lr)

  # prepare data
  train_dataset_shard = chalk_train.get_dataset_shard()
  dataloader = DataLoader(train_dataset_shard, batch_size=batch_size)

  # train on data
  for epoch in range(num_epochs):
    for batch in dataloader:
      output = model(transform(batch)) # revisit
      loss = loss_fn(output, batch["labels"])
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

    # log metrics + checkpoint
    chalk_train.log_metrics({"loss": loss.item()}, step=epoch)
    if (epoch % 5) == 0:
      checkpoint = {
        "epoch": epoch + 1,
        "model_state_dict": model.state_dict(),
        "optimize_state_dict": optimizer.state_dict(),
      }
      checkpoint_dir = chalk_train.get_checkpoint_dir()
      checkpoint_path = os.path.join(checkpoint_dir, f"epoch_{epoch+1}.pt")
      torch.save(checkpoint, checkpoint_path)
      chalk_train.log_artifact(checkpoint_path)
```

### Configuring the Training Job

Once your worker function is defined, configure the distributed training job:

```python
torch_trainer = TorchTrainer(
  worker_function=worker_function,
  worker_config={"num_epochs": 20, "lr": 0.01, "batch_size": 32},
  resource_config={"limit": {"cpu": 4, "memory": "16Gi", "gpu": 1}},
  # require one of `dataset` or `model`
  dataset="already_produced_dataset" # fetches dataset
  # model=MyModel # uses model input bindings to run offline query first
)
```

### Running Training and Registering the Model

Finally, execute the training job and register the resulting model:

```python
import datetime
from chalk.client import ChalkClient

now_string = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
client = ChalkClient()
training_run = client.train_model(
  name=f"my_neural_network_exp_{now_string}",
  trainer=torch_trainer,
)

client.register_model_version(
  name="my_neural_network",
  aliases=["trial_2025_08_20"],
  uri=training_run.result.artifact,
)
```

### Key Features of Distributed Training

The distributed training approach provides several advantages:

- **Automatic Data Sharding**: Chalk automatically partitions your dataset across workers
- **Checkpointing**: Built-in support for saving and resuming training state
- **Metrics Logging**: Track training progress with `chalk_train.log_metrics()`
- **Resource Management**: Specify GPU, CPU, and memory requirements per worker
- **Model Registration**: Seamlessly register trained models for deployment

---

## Next Steps

After training your models, you can:

1. **Register Models**: Use the [Model Registry](/docs/model_registry) to version and track your trained models
2. **Deploy Models**: Load models into Chalk deployments for inference
3. **Monitor Performance**: Track model performance and feature distributions over time
4. **Iterate**: Use the training infrastructure to experiment with new architectures and hyperparameters
